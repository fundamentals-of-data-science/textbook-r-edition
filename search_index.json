[
["index.html", "Fundamentals of Data Science (R Edition) Preface", " Fundamentals of Data Science (R Edition) Andrew Stewart Stephyn Butcher 2020-08-30 Preface This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. "],
["introduction.html", "Chapter 1 Introduction 1.1 Business Statistics 1.2 Defining Data Science 1.3 Data Science is Science 1.4 Big Data 1.5 Data Science Case Studies 1.6 Conclusion 1.7 Review 1.8 Exercises", " Chapter 1 Introduction What if I told you that data science deals with “the collection, analysis, interpretation, presentation, and organization of data”? That sounds about right, doesn’t it? Now what if I told you that was this was the definition of statistics according to the Oxford Dictionary of Statistical Terms. Does that surprise you? You wouldn’t be the first person to wonder why statistics isn’t the “science of data” and why we have this new term, “data science”. In fact, some people claim there is no difference. Nate Silver–a very famous Bayesian statistician–says that data science is just statistics. The economist Hal Varian said, I keep saying the sexy job in the next ten years will be statisticians. People think I’m joking, but who would’ve guessed that computer engineers would’ve been the sexy job of the 1990s? And he goes on to describe what he means by “statistician”: The ability to take data – to be able to understand it, to process it, to extract value from it, to visualize it, to communicate[. I]t’s going to be a hugely important skill in the next decades, not only at the professional level but even at the educational level for elementary school kids, for high school kids, for college kids. Because now we really do have essentially free and ubiquitous data. So the complimentary scarce factor is the ability to understand that data and extract value from it. I think statisticians are part of it, but it’s just a part. You also want to be able to visualize the data, communicate the data, and utilize it effectively. But I do think those skills – of being able to access, understand, and communicate the insights you get from data analysis – are going to be extremely important. Managers need to be able to access and understand the data themselves. — Hal Varian, Flowing Data So why are we talking about “data scientist” and not “statistician”? Would any of you be interested in “data science” if it was called “statistics”? Why is this a book about the fundamentals of data science and not statistics? There are some comical explanations: Data Scientist (n.): Person who is better at statistics than any software engineer and better at software engineering than any statistician. — @josh_wills and: There’s a joke running around on Twitter that the definition of a data scientist is ‘a data analyst who lives in California. — Malcolm Chisholm I think Allen Downey is a bit closer to the truth: Having read The Theory That Would Not Die and The Lady Tasting Tea recently, I suggest the following conjecture: The term “data scientist” has been created to describe what people want from a statistician, but which many statisticians fail to provide because statistics, as a field, spent too much time in the 20th century on problems in philosophy of science, and theoretical mathematical problems, and not enough time on practical applications and the use of computation to analyze data. As a result, many graduates from statistics programs have a set of skills that is not a good match for what the market wants them to do. This market vacuum is being filled by “data science.” That’s my theory, which is mine. — Allen Downey This divide between “data scientist” and “statistician” is also related to the “Two Cultures” theory of machine learning and statistics. Simon Blomberg said, machine learning is statistics minus any checking of models and assumptions – Brian D. Ripley Two Cultures To which Bayesian statistician Andrew Gelman responded, In that case, maybe we should get rid of checking of models and assumptions more often. Then maybe we’d be able to solve some of the problems that the machine learning people can solve but we can’t! Although “machine learning” as such grew out of computer science, many of the same principles and algorithms in other engineering specialties were called “pattern recognition”. In fact, Bishop’s textbook on machine learning is called Machine Learning and Pattern Recognition to cater to both audiences. 1.1 Business Statistics If you listen to the hype, organizations need data science today or they’re going to fall behind their competitors. Organizations today have tons of data and need data scientists to uncover the business value and actionable insights in that data in order to gain–or maintain–a competitive advantage. But, honestly, companies have had data and analysts to pour through that data for decades. The application of statistics to business processes is not new. A business statistics course is often required of business majors in universities worldwide. Here’s a definition of Business Statistics: Business statistics takes the data analysis tools from elementary statistics and applies them to business. For example, estimating the probability of a defect coming off a factory line, or seeing where sales are headed in the future. Many of the tools used in business statistics are built on ones you’ve probably already come across in basic math: mean, mode and median, bar graphs and the bell curve, and basic probability. Hypothesis testing (where you test out an idea) and regression analysis (fitting data to an equation) builds on this foundation. Basically, the course is going to be practically identical to an elementary statistics course. There will be slight differences. The questions will have a business feel, as opposed to questions about medicine, social sciences or other non-business subjects. Data samples will likely be business-oriented. Some subjects usually found in a basic stats course (like multiple regression) might be downplayed or omitted entirely in favor of more analysis of business data. I think the only difference here between Data Science as such and Business Statistics is the sophistication of the tools used. Data Science goes beyond using only elementary statistics and adds complex statistical modeling and machine learning. And yet, elementary statistics is the best starting place whenever you start solving business problems and answering business questions. It is an unfortunate and overlooked truth by those wishing to use the latest, shiny algorithms that most businesses can get by with models based on means, sums and rates. The hard part is working with the data. Nevertheless, the application of elementary statistics to good decision making started well over a hundred years ago. Most famously, Gosset used statistics to improve the brewing process at Guinness in the early 1900’s. In order to do so, he developed an entire theory around working with small samples and published them under a pseudonym, “A. Student”. R. A. Fisher, a famous statistician and contemporary of Gosset’s, spent part of his career analyzing data from agricultural field trials to determine the best fertilizer. Why wasn’t this “data science”? Actually, I think it was. Let’s look at a more modern example, McDonald’s: When you’re as large as we are, we can’t run the business on simple gut instinct. We rely heavily on all kinds of statistical data to help us determine whether our products are meeting customer expectations, when products need to be updated, and much more. The cost of making an educated guess is simply too great a risk. – Wade Thomas in Business Statistics: A Decision-Making Approach That sounds like data science to me. But if we “google” for “data science for X” where X is “fertilizer analysis” we find Monsanto has far surpassed Fisher’s work , when X is beer, we have Empirical Brewery Brews with (Data) Science, and when X is food products, we get, well, McDonald’s in 7 Uses of Big Data in Food and Beverages Industry. 1.2 Defining Data Science Why all the hand wringing? If we’re going to spend time learning how to do something, I think we should probably have general agreement about what that something is. Additionally, anyone trying to find a job as a data scientist or doing data science will quickly be confronted by a mish-mash of job descriptions, some of which have only the most tenuous relationship to each other. I have long since given up the idea that “data science is what data scientists do” because most organizations don’t know what data scientists should reasonably be expected to do and a data scientist may end up doing more than data science. Although it takes this to the extreme, here’s a great example of one such job posting: [Company] is looking for a Data Scientist eager to use advanced analytical, machine learning and data transformation techniques as a means to develop practical tools and analyses that can help solve complex business problems; transforming volumes of data into actionable information. In some instances, you will be using languages like R or Python to employ newer techniques like tree ensembles, neural nets, and clustering to solve long- standing questions. In other instances, it will be your responsibility to come up with solutions for problems that have not yet been identified. In order to do so, the Data Scientist will have to be confident that they can solve a wide- range of problems using a variety of techniques—some known, some new, some yet to be created. My favorite bit is the part I’ve emphasized in boldface: basically, you must solve unidentified problems. How is that even possible? Big Data Made Simple has collected 14 definitions of a data scientist and by extension, data science. Some are pretty good: “Data scientists are involved with gathering data, massaging it into a tractable form, making it tell its story, and presenting that story to others,” — Mike Loukides and some are pretty awful: Data scientist is a “change agent.” A data scientists is part digital trendspotter and part storyteller stitching various pieces of information together. — Anjul Bhambhri and we’ve already seen the funny ones above. Foster Provost in Data Science for Business defines data science as fundamental concepts and data mining as the tools. He quite correctly states that data science is not tools any more than biology is about test tubes. A data science text that focuses on R, Hadoop, and MongoDB misses the point entirely (although you still need to learn how to use those “test tubes”!). Here is a sampling of four of his 12 concepts: Extracting useful knowledge from data to solve business problems can be treated systematically by following a process with reasonably well-defined stages. From a large mass of data, information technology can be used to find informative descriptive attributes of entities of interest. If you look too hard at a set of data, you will find something – but it might not generalize beyond the data you’re looking at. Formulating data mining solutions and evaluating the results involves thinking carefully about the context in which they are used. There are about a dozen or so in his book and we will have cause to revisit them. Still, I’m not sure that 12 maxims truly constitute “data science” but they might certainly inform the discussion. 1.2.1 Venn Diagram of Data Science One of the first attempts to describe what Data Science originated with Drew Conway and his Venn Diagram of Data Science. In many ways, it’s still the best place to start any discussion about Data Science. Figure 1.1: Drew Conway’s Venn Diagram of Data Science Broadly speaking, there are three areas that combine to form Data Science: programming, math &amp; statistics and domain knowledge. Conway says “hacking skills” but I don’t think that hacking skills are sufficient. You really should know what it takes to field production ready software that is tested as much as possible, that has error handling, that is readable both by yourself, your future self (you six months from now) and other data scientists and engineers on your team and in organization. For this reason, I buck the trend that data scientists on a team should use whatever language they’re comfortable with…everyone should use the same language whether it’s R or Python or even Julia. When the data scientist who does everything in Perl goes on vacation–or worse, leaves for a better job–the rest of team shouldn’t have to scramble. I also believe Data Scientists should be in Engineering and not Finance or Marketing and even better if they are embedded in other teams involved in their day-to-day problems and obstacles. Why should you know software engineering and not just “hacking”? It’s true that not everything a data scientist does will end up in production. If you just run a batch job on your local laptop once in a while, if you’re working on a project using a local data set in Jupyter notebook, you may not need every bell and whistle in the software engineer’s tool belt. However, servers world-wide are littered withs scripts that were never meant to make it into production. This makes such “personal” or “one off” scripts a slippery slope. A one off becomes scripted, gets attached to cron and then goes into production all without error handling or tests. You can think of exploratory data analysis as a version of “test first” applied to data, so you should do the same thing with your code: if you can find a simple way to test your code, do so. There’s nothing quite like having to explain to the CEO that the new campaign about to launch is misguided because there was a “bug” in the script. Another area of expertise is “Math and Statistics”. What’s interesting about the diagram here is that Conway views “Machine Learning” as the intersection of programming and math &amp; statistics. In fact, one could say that machine learning is a reification in code of math and statistics. Still, I believe this is too narrow a view. The diagram seems to imply that data science is applied machine learning. This may not be what Conway intends so perhaps there are limits to argument by Venn diagram. There are certain math and statistics that are important as such to data science. These at least include linear algebra, optimization, probability, and statistics. There isn’t otherwise a special “data science math”. I bring this up only because I’ve been asked about “data science math” by past students; there’s just math and some of it is very useful for data science. 1.3 Data Science is Science The final circle is Domain Expertise or Domain Knowledge. In the actual sciences, domain knowledge consists of the theories, concepts and rules of thumb that are generally accepted by practitioners. Acquiring this domain knowledge–along with the ability to add to it–is generally what it means to become a physicist, a biologist or an environmental ecologist. At the fringes, this knowledge is the object of active research but it generally consists of the things everybody “knows”. This domain knowledge gives shape to the creation of new questions, new hypotheses, the design of experiments and the interpretation of results. Your organization will have a body of theories and concepts, rules of thumb just like this but covering a much smaller domain than, say, physics. Not all of it will have been collected scientifically–that’s where you come in. You can think of a data scientist at a particular organization as being an expert in that organization’s domain. A data scientist at Walmart is a “Walmartologist”, a data scientist as Apple is an “Apple-ologist” and a data scientist at Starbucks is a “Starbucksologist”. The key to the successful practice of data science is the marriage of general skills (data acquisition and analysis, statistics, machine learning, programming) to a specific, often very circumscribed, but small domain. This is probably one of the most important aspects of being a data scientist: domain knowledge. Oddly, it is the part of the position that you must get on the job. You will get it from domain experts, stakeholders, and Josephine, who doesn’t appear on the org chart, “but has been here 15 years and knows everything”. Although I have recently started to see data scientist job postings that require experience in a given field such as human resources or healthcare. Conway’s Venn diagram segues very nicely into Max Shron’s definition of Data Science in Thinking with Data that drives this point home: “To me, data plus math and statistics only gets you machine learning, which is great if that is what you are interested in, but not if you are doing data science. Science is about discovery and building knowledge, which requires some motivating questions about the world and hypotheses that can be brought to data and tested with statistical methods.” This is very good quote for several reasons. It may very well be that you are only interested in math and statistics, machine learning. If so, you may find that data science is not your cup of tea. It emphasizes science. It emphasizes that you do not start with data, a point to which we will often return. So, for Shron, there is this sense of doing actual science, of applying the scientific method, on the problems and data available in an organization, in the context of everyday life. He then goes into his actual definition of data science: Data science is the application of math and computers to solve problems that stem from a lack of knowledge, constrained by the small number of people with any interest in the answers. Still, it is an applied science. It is not the quest for information for it’s own sake. Instead, you should always be working on solving a business problem or answering a business question. If I appear to be belaboring this point, it is because I want to make sure there is no room for misunderstanding. The angel you should keep on your shoulder throughout this book is that data science is not synonymous with machine learning, it is the application of the tools of science to everyday problems. The question should not be, “what is the latest version of this algorithm? Is there a better one?” but “if I pulled a data set off the internet right now, could I apply this method to it?”. So data science is just science with the priviso that only a few people have any interest in insights that comprise some company’s understanding of its churn rate as opposed to, say, the general theory of relativity or a cure for cancer. John W. Forman in Data Smart says, Data science is the transformation of data using mathematics and statistics into valuable insights, decisions, and products. which is also good. However, you don’t start with data, you start with a question or a problem. Otherwise, your data science team will not fare much better than the infamous [Underwear Gnomes](https://en.wikipedia.org/wiki/Gnomes_(South_Park): Hire data scientist. ??? Profit You start with people, the domain knowledge and their problems and you end with communication and increased understanding. Even in the not so grand exploration of a variable, “daily sales”, Data Science is not just the rote execution of code and plotting charts. You should always have a reason for why you did something and document it. Communication is central. So what is our working definition of data science? While there are going to be exceptions, I believe the definition with the broadest applicability includes: Communication - the identification of problems and questions as a goal, iterative communication on the progress towards reaching that goal, and the communication of the solution or answers. Decision making is a key aspect of this as well. Problems are solved and questions answered because someone wants to do something. Data - problems and questions arise in the context of a real world processes. Data are our observations about those processes and the raw material for our analyses. Modeling - whether it is statistical or machine learning, the central focus of data science is to build models of real world processes to answer questions or solve problems. It is no accident that most statisticians who say data science is just machine learning are Bayesian statisticians. Bayesian statistics emphasizes model building. In a sentence, something like, “data science is application of modeling to data to solve problems or answer questions in support of decision making”. This definition also emphasizes something important: data scientist is a supporting role. 1.4 Big Data At this point, you might be wondering. What about big data? There so much hype surrounding “Big Data Science”, that Dan Ariely quipped: Big data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it… – Dan Ariely The results in Analyzing the Analyzers seem to back Ariely up. Although “Data Science” and “Big Data” are often spoken of in the same breath and usually just a breathlessly, the survey showed that most data scientists don’t work with anything like “big data” on a regular basis: Part of the problem here is that the concept of “Big Data” has drifted. And now some people are saying that it’s all just “data” now–there’s no such thing as “big data” and a lot of that has to do with technological improvements. For a good discussion of the issues, read Don’t use Hadoop - your data isn’t that big by Chris Stucchio. It starts with a funny–but all too common story–of a client that wanted to use Hadoop on 600Mb of data. The upshot: if your data is under 1TB, modern computers (8+ cores, SSD, 16Gb of memory in a laptop), databases (even SQLite!), and hard drives (external hard drives come in 4TB models) and Python are more than sufficient for your analytics needs. This doesn’t necessarily hold for your production needs. 1.5 Data Science Case Studies In order to get a feel for Data Science, we’re going to present a few case studies in Data Science. As we go through the main chapters of the book, you should come back here and see how much more you understand about what went into these examples of Data Science. As you read or watch each the following examples of Data Science, take note of anything that interests you. You should also answer the following questions: What question where they trying to answer? How did they approach the question? Where did they get the data and what technique did they use? 1.5.1 Signet Bank What can be gained from classification? There are many iconic stories of how forward thinking companies anticipating business issues before they arrive – and then take action. My favorite is story Signet Bank, whose credit card division was unprofitable, due to “bad” customer defaults on loans and “good” customers being lost to larger financial institutions who could offer better terms and conditions. The answer, revolutionary at the time, was to apply classification to their customer data. They separated the “Bad” from the “Good”, cut the “Bad” ones loose and nurtured the “Good” ones with offers and incentives. Today, we know them as Capital One. Data Science Foundations – Classification and Regression But this isn’t the whole story. Signet Bank didn’t have data with which to develop a classification algorithm so it had to generate it. How did it generate the data? By offering customers random terms. They accepted losses for 5 years in order to collect data from which they could learn what made a good customer that stayed and what made a bad customer who you shouldn’t give a credit card to in the first place. 1.5.2 Target This particular story received national attention. Basically, a father called Target to complain that his daughter was receiving targeted maternity coupons and it was distressing because his daughter wasn’t pregnant. The manager apologized. A few days later, the father called back. His daughter was indeed pregnant. He apologized. How did Target know that his daughter was pregnant? Well, they didn’t know but they had a good inkling. Target collects data on all of its customers, assigning to each of them a “guest id”. Looking at actual maternity purchases, you can back up and see what did this person purchase last month? Two months ago? Three? As [Target statistician Andrew] Pole’s computers crawled through the data, he was able to identify about 25 products that, when analyzed together, allowed him to assign each shopper a “pregnancy prediction” score. More important, he could also estimate her due date to within a small window, so Target could send coupons timed to very specific stages of her pregnancy. One Target employee I spoke to provided a hypothetical example. Take a fictional Target shopper named Jenny Ward, who is 23, lives in Atlanta and in March bought cocoa-butter lotion, a purse large enough to double as a diaper bag, zinc and magnesium supplements and a bright blue rug. There’s, say, an 87 percent chance that she’s pregnant and that her delivery date is sometime in late August. You can find out more by reading How Target Figured Out A Teen Girl Was Pregnant Before Her Father Did. 1.5.3 Walmart Walmart also collects information on their customers. In 2004, as Hurricane Frances was about to make landfall, Walmart started analyzing the purchasing patterns of customers prior to Hurricane Charley. The experts mined the data and found that the stores would indeed need certain products – and not just the usual flashlights. “We didn’t know in the past that strawberry Pop-Tarts increase in sales, like seven times their normal sales rate, ahead of a hurricane,” Ms. Dillman said in a recent interview. “And the pre-hurricane top-selling item was beer.” Thanks to those insights, trucks filled with toaster pastries and six-packs were soon speeding down Interstate 95 toward Wal-Marts in the path of Frances. Most of the products that were stocked for the storm sold quickly, the company said. Common sense says that things like toilet paper, milk, eggs and bread are always the first to go in an emergency but discovering an increased demand for Pop-Tarts and Beer is definitely interesting. We also see another example of how insights lead to decision making. It is neither knowledge for knowledge’s sake nor an opportunity to apply the latest machine learning algorithm. You can find out more by reading What Wal-Mart Knows About Customers’ Habits. 1.5.4 Obama Campaign The first Obama campaign was already “data driven”. In the 2008 presidential election, Obama’s targeters had assigned every voter in the country a pair of scores based on the probability that the individual would perform two distinct actions that mattered to the campaign: casting a ballot and supporting Obama. These scores were derived from an unprecedented volume of ongoing survey work. For each battleground state every week, the campaign’s call centers conducted 5,000 to 10,000 so-called short-form interviews that quickly gauged a voter’s preferences, and 1,000 interviews in a long-form version that was more like a traditional poll. To derive individual-level predictions, algorithms trawled for patterns between these opinions and the data points the campaign had assembled for every voter—as many as one thousand variables each, drawn from voter registration records, consumer data warehouses, and past campaign contacts. By the re-election campaign, however, they had grown more sophisticated and moved to models of individual behavior. They wanted to find persuadable voters. While the Romney campaign tried to match the Obama campaign’s data science team, they never got beyond the question, “is all this advertising working?”. The Romney team was one-tenth the size of Obama’s. As we all know, the application of data science to elections is a controversial topic following the 2016 election and the Brexit referendum. Both Cambridge Analytics and Facebook have come under increasing scrutiny for their different roles in the application of data to those elections and data “breeches”. Events like these (as well as Walmart and Target) are bringing ethical concerns to the fore. The most visible result of this concern was the passage and implementation of the European Union’s General Data Protection Regulation GDPR. Many believe something like GDPR will come to the United States and international firms have started general GDPR compliance as a matter of convenience. You can find out more by reading How President Obama’s campaign used big data to rally individual voters. 1.5.5 Identifying Place Foursquare is a mobile app that allows you to share location information with friends by “checking in”. Blake Shaw of Foursquare made a presentation at Data Gotham 2012 that showcased the interesting questions that could be answered with this kind of location data. He first started by animating the check-in data for New York City (Manhattan actually). This showed interesting but not unexpected patterns of people checking in at work and transportation hubs, eateries for breakfast, work and then eateries for lunch, work, then eateries and home for dinner and the rest of the night. The kinds of questions that can be asked of this data include: 1. What is a place? He showed check-ins that identified “Central Park” and “JFK Airport”. Note that this is people checking-in and saying where they were. This process could be used to define a “place”. 2. How do check-ins vary over time? Next he shows charts that show check-ins over time for three places. These charts show how these places exist in time relative to their check-ins. The coffee shop has a different pattern than the casual eatery which is still different than the more formal restaurant. However, there are interesting patterns…the eatery is busy late night on Fridays and Saturdays and the restaurant is busy at midday on Sundays…they serve brunch. 3. What places should be recommended for a weekend? It turns out that stadiums, flea markets, dim sum, and pool halls are much more popular on the weekends. 4. What are the characteristics of a neighborhood? For this analysis he compares Soho to the East Village and shows how there are more check-ins at offices in Soho and more check-ins at bars in the East Village. 5. What neighborhoods are similar? He demonstrates using this same information to identify neighborhoods that are similar. All of this is interesting information. It appears that Foursquare has finally started monetizing this data but that is the central challenge. You can have a lot of interesting data and frame questions about it but are the answers something you can act on? This is not to say that data can’t be used for something other than to make money but the goal is usually some sort of insight that influences decisions. These decisions might just make people happier. You can find out more by watching Big Data and The Big Apple. 1.5.6 Pruning Trees in NYC This next case study is interesting because it was done by someone at Media6Degrees, Brian Delessandro, during “donated” time. The idea of donated time has made some headway into some companies. Basically, it’s an opportunity to use company resources to give back to the community. It’s a bit like open source, except you are doing analysis. Although Delessandro is a work-a-day data scientist interested in his company’s bottom line, Brian’s decision to contribute was inspired by Jeff Hammerbacher’s observation that, The best minds of my generation are thinking about how to make people click ads…and that sucks. and the desire to contribute to the social bottom line. As a side note, Jeff Hammerbacher left Cloudera to work in data science for medicine, specifically, depression. The particular study that Delessandro was involved looked at the efficacy of “block pruning” in New York City. During a typical day, there are very few fallen branches and trees in New York City. This makes the typical workload easy to plan for in terms of staff and equipment. During storms, however, there is typically a significant increase in fallen branches and trees. This debris creates a public safety hazard (and inconvenience if they’re blocking your road). Block pruning involves sending crews to specific locations throughout NYC to trim trees to make the aftermath of a storm look like a typical day. The question is, does this work? The best way to answer this question would be to pick blocks at random separating them out into control (no block pruning) and treatment (block pruning) and monitor the blocks of the course of the next year or so. However, as in many medical trials, this approach creates an ethical problem because the City is purposely not trimming trees in certain areas and allowing hazardous conditions to exist. This rules out a traditional A/B test. However, it turns out that NYC has a census of trees in the City (“The Urban Forest”) that contain information about every tree in the city: species, age, location, etc. They also have historical data on every maintenance action on every tree. After aggregating this data to the block level, Brian was able use this observational data to build a model that related treatment (block pruning) to the outcome (work orders in the following year). This particular study is interesting because of the presence of confounding factors. A confounding factor is one that affects both the treatment and the outcome. For example, blocks with more trees are more likely to have large tree and are thus more likely to get pruned. Additionally, the crews have latitude in doing “surgical” pruning as they travel the city. The original results actually show that pruning makes the city more hazardous! Using a method designed to control for confounding factors, the data did show that pruning reduced future hazards by 13%. Observational studies like this are very common in situations where controlled testing would create ethical problems. For example, epidemiologists determined that cigarette smoking is hazardous not because they did an A/B test, randomly assigning people to control and treatment groups (and forcing the treatment group to smoke 1, 2, 3 packs a day). Instead, they were able to use observational data to determine that there was a strong causal link. This research is also an excellent demonstration of why you must have domain knowledge when doing data science. You have to know where your data comes from and the actual, real world process that generates it. My favorite quote of the presentation, Data scientist salaries are good but they’re not country club good. You can find out more by watching Change the World in a Day. 1.5.7 The Information Architecture of Medicine is Broken This research by Ben Goldacre represents some interesting data science detective work in the field of medicine. The cruz of his research is this. He was interested in the efficacy of a certain anti-depressant (“the drug”). In the US, drugs are only approved by the FDA after they have undergone a certain number of clinical trials proving their efficacy. We will have a lot to say about “efficacy” later but in the case of medicine this goes by the standard of “clinically significant results”. For depression there is a survey that sufferers take that determines a depression score. The clinically significant result here is a lowering of one or more points on the scale. As you might imagine, the result of one study might show that the drug lowers depression by 1.2 points on average. Another study might show that that the drug lowers depression by 0.2 points on average. Still another study might show that the drug lowers depression by 1.3 points. With a sufficient number of studies, we can plot these points as a histogram. By the Central Limit Theorem, this histogram should be relatively bell-shaped or have a Normal distribution. What Goldacre found was that the actual plot looked like a bell curve cut in half, showing only the good results. How is this possible? As previously mentioned, in order for a medication to be approved by the FDA it must undergo a series of clinical trials. The catch is that pharmaceutical companies can start trials and stop them when the results don’t seem to be going in their favor. They do not need to report the existence of those failed trials. This explains why the results are not normally distributed. As a result, Goldacre and others have tried to start capturing information about all clinical trials. You can find out more by watching The Information Architecture of Medicine is Broken. 1.5.8 Love in the Time of Data Daniel Chapsky is a Data Scientist at Snap Interactive, which produces a dating app called AYI (“Are You Interested?”). AYI is a recommendation “engine” (model) built on top of Facebook’s social graph and other information. One of the interesting things about this presentation is the use of data-driven personas. Personas are very common in user experience/user interface (UX/UI) design. By adopting a particular persona (“manager”, “tech lead”, “data scientist”) designers can go through an application and see how well the application serves the needs of that particular persona. AYI developed data driven personas of their customers using clustering. The presentation talks about three different personas, “Rupert”, “Pam”, and “Blane”, who each want very different things from a dating site and interact with its features in different ways. This case study also demonstrates some central problems in working with data. The most central here is feature engineering. How do we take the data available from Facebook and external sources and turn it into actual features in a recommendation engine? For example, Facebook interests can be noisy for some applications (for example, “Domino’s Pizza”, which is probably not a good dating interest) and too specific as well (for example, specific artists instead of “Jazz”). AYI tackled these problems by using external data to generate higher level taxonomies or labels. Another aspect of feature engineering involves using the social network. Certain personas (“Pam”) are more likely to respond to a message if they have a friend in common (a feature). They are also more likely to have a lot of friends (a feature). Using the social network and AYI membership, AYI is able to better match “Pam” with people she is more likely to respond to. Finally, in a dating app, attractiveness is going to play some kind of role. Through exploring their data they noticed that women are picky regardless of age and men are pickier as they get older. They then layered these inferences (common interests, friends of friends, pickiness) onto the social network and created a recommendation engine. Note that since this presentation, Snap Interactive rebranded to PeerStream. They are one of the largest social media companies with chat, video and dating apps that “piggyback” on the social network. It appears that AYI became “FirstMet”. And for you GenXers out there, they have “0ver50” as well. One has to wonder how they’re faring in a post-Cambridge Analytics world. You can find out more by watching Love in the Time of Data. 1.5.9 Booz/Allen/Hamilton Data Science This is a set of case study presentations by Booz/Allen/Hamilton (BAH) given at the Data Works Maryland Meetup. During the introduction, BAH discussed how they organize their data science teams. Since BAH is a consultancy, it is a little bit different than other companies and more project driven. For any given data science project, they assign a software engineer, data scientist and domain expert. Instead of trying to find a single unicorn with all the skills, they build unicorn teams. There is some interesting information in the presentation that we will return to but our interest here is in the lightning talks showcasing case studies in data science. Malaria kills an estimated 600,000 people a year and nearly 2 million are infected. Poor countries are hit especially hard. Africa has an interesting mix of good and bad infrastructure. In terms of good infrastructure, Africa has a well developed cell tower and mobile phone system. There was a malaria outbreak at a teak plantation. The typical response is to treat the local bodies of water with chemicals to kill the mosquitos but that didn’t seem to be working this time. Someone made the observation that a large percentage of the workforce of this plantation was composed of migrant workers. Accessing anonymized cell phone data, they were able to create a map of where all of these people had been over the previous weeks. It turned out that many of them had traveled near Lake Victoria, hundreds of miles away. When Lake Victoria was treated, the malaria outbreak at the teak planation stopped. Vehicle theft is a major problem in metropolitan areas both in terms of property loss and public safety. Using crime data and plotting it on a map of San Francisco, a data science team was able to identify several hotspots in the city for vehicle theft. Concentrating on a single hotspot they saw that the hotspot was surrounded by three parks that made foot access (to and from) very easy. Although one might think most vehicle thefts occur in the wee hours of the morning, looking at the data over time for this hotspot, they identified 9-10pm as the peak hours for crime. Using this information as a model, the city deployed police to these specific locations at the specific times indicated by the model. In response, there was a shift in the hotspots. Cancer is a major health problem throughout the world for both poor and wealthy countries. For those cancers with successful treatments, differential access to medicines can mean the difference between high and low mortality rates. In this particular case, the medicine used to treat the specific type of cancer (neither the medicine nor cancer were named) is the result of a biological process. Biological processes are difficult to control and have quite a bit of variation. The goal of the company was to see if data could be used to decrease the variability of the process and increase the yield, making more medicine available and lowering the cost. The available data was mostly time series: records of sensor measurements (pressure, temperature, etc.) over time as the process ran. There was 10 years worth of the data, which measured in a several terabytes. The team’s theory was that runs with similar profiles–the movements of measurements over time–would have similar yields. In order to measure similarity for time series, they used dynamic time warping. In the end they were able to identify the conditions that lead to less variable, larger yields. Unfortunately, these videos were removed in late 2018. Hopefully these case studies will have given you a flavor for what can be done with data science. 1.6 Conclusion Although there isn’t complete agreement about what Data Science is, there are enough commonalities in the different definitions to define a broad set of skills that comprise doing data science. These skills include “hard” skills like programming, statistics, and machine learning as well as “soft” skills like communication and domain expertise. For the purposes of this text, we will define data science to be “the application of modeling to data to solve problems or answer questions in support of decision making”. There are three main components here: problems/questions/decision making, data, and modeling. We also presented a number of Data Science case studies. By the end of this text, you should have a greater understanding of what went into each of these case studies and be able to do similar analyses. 1.7 Review What is the definition of statistics? Why is it difficult to define data science? What is the working definition of data science for this text? 1.8 Exercises In Analyzing the Analyzers, the authors identify four different kinds of “data scientists”. Skim through the report. If you plan on becoming a Data Scientist, what kind of Data Scientist will you be? Because of the interdisciplinary nature of Data Science, (ideal) Data Scientists described has having a T-Shaped skill set. Looking at the skill sets documented in Analyzing the Analyzers above, what breadth skills do you think are lacking and what skill would you like to develop as your “depth” skill? Kaggle used to provide an interesting list of Data Science use cases broken down by function and industry. Are there any interesting use cases in your function/industry? It’s interesting to note that many of these use cases can be found in a typical business statistics book. Find five data scientist job listings on an employment site. What’s do they have in common? What’s different? (You probably shouldn’t do this at work…). Find your own case study for data science (not just applied machine learning or “Artificial Intelligence”). "],
["process.html", "Chapter 2 Data Science Process 2.1 The Process 2.2 Side Note on Data 2.3 Stages of Data Science 2.4 Agile Data Science Pyramid 2.5 ASK 2.6 CoNVO 2.7 CoNVO Examples 2.8 Resistance 2.9 Conclusion 2.10 Review 2.11 Exercises 2.12 Additional Resources", " Chapter 2 Data Science Process In this chapter we will discuss “The Data Science Process”. The Data Science Process is a framework that identifies the key steps to completing a data science project. This Process includes with identifying problems in your organization by talking to various stakeholders, obtaining and scrubbing data, data exploration, modeling and finally “reporting” your results in some form that affects the operation of your organization. We will use the “Green Cloud” model of Data Science in these notes. However, because different organizations have differing degrees of data science experience, we will also talk about stages of data science and the data science pyramid put forth by Russell Journey. We will then look at the first step in the “Green Cloud” model of Data Science, ASK, and explore at least one way we can improve communication at the onset of a data science project. Finally, we will talk a bit about data science teams and data culture as the larger context in which the data science process unfolds. There’s a lot more to Data Science than algorithms. 2.1 The Process Unfortunately, mere algorithmic cleverness is not sufficient to ensure that a data science project succeeds. If you can program a deep learning neural network using a redstone computer in Minecraft, but you cannot properly identify a problem with business value in your organization, your data science team will ultimately fail. If you can conduct statistical analyses but you cannot write clearly about your results, your results will go unused and your insights unheeded. In the Introduction, we defined Data Science as: the application of modeling to data to solve problems or answer questions in support of decision making and we emphasized that data science was more related to applying the tools and mindset of science to everyday problems than with “working with data”, whatever that might mean. We also emphasized that you will come to an organization with general skills but they will have their own, specific problems. Biologists spend years learning the ins and outs of biology; you will have weeks to become an expert in your organization’s processes, terms and domain knowledge. The Data Science Process is an idealized view of the steps needed to go from the start to the “end” of a Data Science project. The quotation marks signify that such projects rarely truly end. These steps include things we generally think about (getting the data, building models) and those we do not (talking to people about the problem we’re supposed to solve). I think the two most important guiding principles in Data Science are: Communication - solve the right problem for the right people and talk about expectations and results constantly. Explain things clearly. You need to be good at both written and oral communication. Simplicity - follow the agile development dictum and start with the simplest thing that could possibly work. Focus on solving problems that add business value and not algorithms. A quick search for “Data Science Process” will bring up any number of diagrams that differ in both fundamental and subtle ways. If we search on Wikipedia (the final authority on everything, right?), we see a typical diagram of the “Data Science Process”: Figure 2.1: The Data Science Process (source) This is a very typical rendering of the Data Science Process. It starts with the collection of raw data which is then processed. This processed data is cleaned into a form suitable for exploration. As you explore the data, errors and shortcoming may be detected in which case you go back to processing data. When the data is cleaned sufficiently, you progress to models and algorithms. Depending on the use case, the results of your modeling are either deployed as a data product (for example, a recommendation algorithm) or reported back to stakeholders who use the insights discovered to make decisions. This characterization is fine as far as it goes but it falls a bit short by starting with the data. This gives rise to unreasonable expectations on all sides. First, stakeholders and decision makers think that just throwing data at “Data Scientists” will make money. Second, data scientists, perhaps with little training outside of academia, think that just sifting through data for six months will make a valuable contribution to the organization. This is the path to “nothing appears to succeed and everyone is laid off, Data Science is hype.” and harkens back to those nefarious and misguided Underwear Gnomes from South Park. As Forrester reports, 99% of companies think data science is an important discipline to develop yet only 22% of companies are seeing significant business value from data science. I have personally witnessed the dissolution of several data science teams and heard of several companies that either laid off entire teams of data scientists or were not hiring data scientists until they had “figured out” their data science story. I will return to the “fork” in this process that generates either reports in support of decision making or data products in support of business in a moment. The Green Cloud model is more explicit about the starting point for the Data Science process: ASK - asking a question (or asking for problems to solve). Specifically, it asks you to identify a goal for the data science project. It also asks you to think about what you would do if you had all the data at hand. We also ask what kind of problem we’re going to solve: prediction or estimation (which I would call “explanation”). More generally, we need to talk about our ideal solution to the problem and what kind of model it might entail. Although not show here, we may need to address any ethical and or legal issues associated with solving this problem. We will talk about ways of framing the ASK step later in this chapter. The GET step involves obtaining data which may be inside or outside the organization. We also need to identify which data are relevant. This is also the place to address privacy issues related to the data (as opposed to the problem or question). “Extract, Transform, and Load” (ETL), “Data Munging”, and “Data Wrangling” are often associated with the GET step. Data Scientists often spend 80% of their time in this step (although it does blur into the next one a bit). The EXPLORE step involves looking at the data and trying to identify anomalies and problems. This step is often associated with Exploratory Data Analysis (EDA). The first pass at EDA might involve looking for anomalies and errors in the data. When such problems are discovered, we often have to go back to the GET step. The second pass at EDA will concentrate on visualization and the identification of patterns that will help us understand the data and make our modeling more effective. The MODEL step is where we build representations of the data that help us solve the actual problem or answer the question. Models can be simple, if we want to know what our current churn rate is, or complex, if we need to identify entities in images. Simple models using averages, rates, and linear models are often all that is needed here, for most organizations. The REPORT step is where we communicate our results back to the organization. Here we try to connect the insights we have unearthed back to the decision makers in the organization whether it is through a report, an email or a dashboard. Although it might be slightly controversial, I generally identify the creation of data products with machine learning engineering. This doesn’t mean, however, that some data scientists won’t be doing some machine learning engineering. My perspective on this topic is the result of several years of working on recommendation engines. One of the most frustrating things about working on recommendation engines–for me–was that we never learned anything about the customer’s preferences. Sure enough, we picked algorithms, engineered features and deployed recommendation engines. We also conducted A/B tests to see which one was best. But none of the techniques we used, from selecting models, obtaining data, identifying features, training and deploying models, and evaluating the results was outside what you would learn in a machine learning course. And, at the end of the day, we had no insights into our customers’ behaviors (although we did make more money, which was good). And this was frustrating to me because I wanted to learn cool things about our customers. Put differently, if this is data science then every use of machine learning is data science and that doesn’t seem right. This is not to say that data scientists won’t do applied machine learning or what I would call “data products”. But this goes back to the idea (which I reject) that data science is what data scientists do. Data scientists may do data science and machine learning engineering. In any case, this is a provisional distinction that may eventually disappear. As indicated by the arrows, we will very often have to go back and revisit previous steps. There are some very “tight” loops in the GET, EXPLORE, and MODEL steps because, as we will see, your modeling may reveal deficiencies in the data you have at which point you need to go back and apply different transformation or get new data, all of which will need to be explored. At the end, we may have more questions or problems that need solving and the process starts all over again. As we will see, it’s best to keep focused on a single problem and “solve” it then come back to the questions and problems raised. 2.2 Side Note on Data We’re going to talk about data a lot in this text. Organizations have always had databases and we already mentioned that Gosset was trying to improve brewing using statistics well over a 100 years ago so so you might ask yourself, why are Data and Data Science such a big thing now? The flippant answer is that no MBA has ever understood what their business statistics course was trying to tell them. And in a world of specialization, this is, perhaps, not unexpected and perhaps it was never realistic. After all, MBAs and Business Majors take accounting classes as well but they still hire CPAs. A better answer, I think, hinges on four factors that converged over the last decade: Ubiquitous computing. Cheap data storage. Logging. Success of Machine Learning. Ubiquitous computing is huge for a number of reasons. Your iPhone has more computational power than the original Cray. Your laptop is beyond what anyone could even dream of a few decades ago. This means that organizations–and individuals–have more computing power at their disposal than ever before to run simulations, learn a programming language, do machine learning, etc. It used to be very expensive and time consuming to do any of these things. Cheap data storage means we can just save everything and look at it later. This has possible legal and ethical ramifications which we should not fail to recognize and discuss but in general, this is a great boon to organizations. Logging is very important for the rise of data science which is a bit weird because it’s more or less a side effect of the rise of the internet. By logging every interaction on a webserver, we were inadvertently taking measurements of human behavior that we could never have done before. Let’s compare two scenarios. In a brick-and-mortar grocery store, you go up and down the aisles the way you want, you look at products, you pick some up, some are put in the cart and others are put back. In the days before UPCs, you just checked out. In an online store, all of that is controlled and audited. You are shown a particular order of products. Your search for specific products is logged. Every page you visit is logged. Anything you put into your cart is logged and everything you take out of your cart is logged. Even for non-ecommerce situations, there are important differences between old style computer systems and ones with logging. For example, in old style computer systems, when you update your address in a database, the old address is typically gone. When you update your marital status, the insurance company’s database forgets that you were ever single. The logs, on the other hand, remember everything. They are records of event streams that are much more interesting. Consider a log that records everything you look at on Amazon and the cost of doing the same thing at the local Safeway. This is actually why the bank account example in Object Oriented Programming is so misleading. Banks keep records of transactions. Your account balance is a derived value (although there are cached starting points that they work from, normally coinciding with statement periods). Finally, the broad success and availability of Machine Learning algorithms is finally making it possible to include them in everyday projects. This wasn’t usually the case with Artificial Intelligence (Does anyone know what an “Expert System” is anymore?). Of course, data science isn’t just machine learning and machine learning may not even be the most important part of data science. 2.3 Stages of Data Science Data Science teams, projects, and even models can go through different stages. Do we have the data for this project? Do we have the expertise on this data science team? What kind of model do we need for this problem? There are lot of options and the right option is influenced by a host of factors. These factors may include: The appropriateness of a technique or algorithm to the problem. Some approaches really are overkill. If a stakeholder wants to know about purchase rates, you don’t need to train a deep neural network. On the other hand, if you have an application that is heavily dependent on “machine” vision and recognizing shapes in images, you may very well need to start with the state of the art. The availability of data for the problem. We often talk about how much data is produced everyday but we don’t often stop to consider if any of the data is actually useful or if the right data is being collected. You may have a problem for which your organization is not collecting data (and needs to start) or has been collecting the wrong data (and needs to fix or improve the data being collected). Historical approaches to the problem in your organization. In the past, a financial analyst has always just calculated your performance metrics but now you want to get more sophisticated. You may want to change the metrics, start calculating confidence intervals for them and even projecting them. On the other hand, you may come to a problem that has already seen great improvements from linear regression and your task is to see if a new model or new features will improve performance. On new problems, you should start simply but on historical problems, you have to take into account the work that has gone before. The maturity of the data science efforts in the organization. The maturity of data science in the organization affects your choices in a variety of ways. With fledgling data science teams, you often need to concentrate on breadth. With so many different problems, the return on investment (ROI) to data science is often highest at the start. That is moving from a non-data driven approach to a data driven approach is often has the biggest impact on the problem. Moving to more sophisticated models can bring incremental increases in returns but often not as big as the first models. I saw this first hand at Company X. We struggled with building complicated models we “knew” should work but didn’t. At one point we switched to a very simple model and got a huge lift over the “conventional” practice (in data science, “lift” is the differential increase in something like review; you will often see “lift” mean “effort” in product management circles. They don’t mean the same thing at all!). And although we spent months looking for better models, most of the time they were only better than the first model by small amounts. Because of the effort involved in switching models, we required a new “winning” model to have a lift of at least 3%. We never replaced the simple model. So we concentrated on breadth, bringing data science to other parts of the company in order to get those large initial payoffs. As the data science efforts in an organization matures, there can be more room for specialization. In “One Data Science Job Doesn’t Fit All”, Elena Grewal talks about this at AirBnB: We started off as the “A-team” — Analytics team — with the first hire an “Analytics Specialist.” In 2012, I was hired as a “Data Scientist.” Later, we hired a “Data Architect,” to tackle data quality, then a “Data Analytics Specialists” to help solve gaps in data access and tools. Then we saw additional needs in machine learning so we hired “Machine Learning Data Scientists.” These title evolutions were both reactions to team needs and also to the competitive landscape. We became the “Data Science” function in 2015, though we still use “A-team” because it’s fun and has a history we value. She goes on to explain how Data Science has evolved into three tracks: Analytics, Algorithms, and Inference. The Analytics track is focused on asking questions and surfacing answers in the form of dashboards. The Algorithms track is focused on applying machine learning to improve business products. The Inference track is focused on improving decision making and measuring the impact of initiatives. It is interesting to observe that in her post, Grewal notes that people in the Analytics track often feel like they are undervalued relative to those in the Algorithms track even though they At Company Z, where I worked, we had a similar tension when the Business Intelligence (BI) team paired up with the Data Science team. I often think of the Analytics and Inference tracks as “BI++” (after C++). At Company Y, there was a similar division between those who worked on Inference and those who worked on Algorithms. I am aware that this division exists at many other organizations as well. AirBnB is relatively mature, however. In an organization just getting started with data science may either focus just on Inference or have a small team that does Inference, Algorithms, and Inference. This is somewhat biased towards internet companies. Non-tech companies may not have an Algorithms track at all. Ganes Kesari talks more about the evolution of Data Science teams in “What are the 3 Stages where Data Science Teams Fail?”. First, he echos a sentiment that will be a major theme throughout this text: Going by any analyst estimate, hundreds of billions are being thrown in by companies to solve problems with data. The key ask is to draw actionable insights that can drive business decisions. Building of predictive models is the top-of-mind recall with the very mention of the word, ‘analytics’. However, considerable business value in data science comes with the right application of exploratory analytics or statistical techniques. While AI or Deep learning have their rightful place, they are not silver bullets for Business ROI to every data problem. But he continues, discussing the evolution of data science teams from “Makeshift Camp”: Similarly, too much of preparation for an ideal mix of skills could lead to analysis-paralysis. Onboard generalists, people who can cover many of the needed skills in analytics (say statistics, programming and information design), even if only to a limited depth. The need is for survivors who flourish in scarcity, wear many hats and instill dynamism to solve any given challenge. to “Thatched House”: Having won small victories with the initial team and established a purpose, the data science team can start fanning out into adjacent usecases. Slowly expand the scope of problems addressed and deepen partnership with users. Initial pilots can now mature into longer initiatives spanning a quarter or year. Showcase enhanced ROI to justify the next level of investment needed. While things may start to work in one’s favour, avoid over-committing in this interim stage. Start specialising by investing in few deeper areas (say Sales analytics, NLP), while continuing to be shallow and get job done in others (say design). to “Palatial Home”: As an evolved entity, the data science team is essentially a mature business unit now. With specialised domain expertise and grasp over all key data science skill areas, the team is now ready to handle sufficiently complex problems, across a wide breadth of areas. No longer faced with existential challenges, the team’s mandate can be deeply woven into long-term business objectives of the stakeholders. Teams could be structured with a vertical alignment, or as technical centres of excellence alias horizontals, or maybe along a hybrid, matrix structure which goes in between. I have seen firsthand (and heard many times second hand) stories about data science teams that did not survive their existential challenge. If your organization does not currently have a data science team or if you’re part of a business unit spinning up their own data science capabilities, do not necessarily look to the Facebooks and AirBnBs of now for inspiration but how their Data Science teams started out. Booz Allen Hamilton describes these Stages of Data Science maturity in a slightly different but useful way. They even go down one level…the first state of data science is collecting data and having it available. This is another landmine in the rush to having data science at an organization. If you do not have the data or it is not accessible to the data science team, your data scientists will be twiddling their thumbs. And while data scientists should have the basic skills needed to obtain data from internal databases and external APIs, a lack of data and or data infrastructure can severely limit what the team is capable of accomplishing. You cannot hope to long endure if you have a automatically generated weekly report that runs from Jane’s laptop and requires constant supervision. Figure 2.2: The Stages of Data Science Maturity (source) The first stage of data science is Collect. The organization needs to collect (internal) or obtain (external) the data. The provenance of the data needs to be maintained (where did this data come from?). Additionally, in this stage you are going to need the pipelines that move data from place to place. At first these might just be data scientists with the requisite skills. As the effort matures, however, this can evolve into a data infrastructure that moves data from production and external sources to data warehouses. The provided example is also illustrative of the idea of data enrichment. In this case, we want to see if there’s a relationship between sales (internal data) and weather (external data). The weather data enriches the sales data. The second level, Describe, was alluded to above. This is always where you start. It is foundational not only because of Exploratory Data Analysis but also because simple rate calculations–with or without data enrichment–can be very illuminating. The third level, Discover, involves either visualization or clustering. The fourth level, Predict, involves anything that’s a simple as linear and logistic regression all the way to random forests and neural networks and, finally, deep learning. We will spend a lot of time on the simpler, interpretable approaches in this text. The fifth level, Advise, bring the previous levels together along with A/B testing and experimental design. Note that these stages can apply to specific projects as well as the entire data science effort of an organization. 2.4 Agile Data Science Pyramid Agile Data Science is a wonderful book by Russell Journey that is hard to classify. It takes a practical and hands-on approach to building an entire data science project iteratively–as you expect from something with the term “Agile” in the title. It applies the “Agile” philosophy to doing data science. The Manifesto for Agile Software Development states: We are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value: Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan That is, while there is value in the items on the right, we value the items on the left more. Taking these four times together, the general idea was that software engineers would consult directly with end users to build software with the features they actually wanted to do the work they needed to do. The software would be built iteratively starting with the basics and enhanced as additional “user stories” were uncovered and implemented. Journey wants to apply this method to data science. As a result, some of the focus shifts. For example, data science as science does not necessarily have predetermined results that can be managed in a predictable way. Although it is widely recognized that building software is not like building cars, you can still get an approximate estimate from a programmer as to when a new feature will be added. I have been asked by product managers when I thought I would find a more successful model! This isn’t much different than asking when I’ll find that cure for cancer. As a result, data scientists need to be very communicative about their process and progress. If you shoe horn data science into a software engineering model, the stakeholders will often be frustrated. I remember being the only data scientist at a smaller company and having to report each morning at the “stand up” (meeting), what I had done and what I was doing. While the software engineers had different bugs and features every day, I was almost always working on the same thing. The method of reporting didn’t match the work. However, there were also some lessons I hadn’t yet learned about communicating progress over results. Journey formalizes the iterative application of data science to a problem in the Agile Data Science Pyramid: Figure 2.3: Agile Data Science Pyramid With start with the base, records. In this case, you are building the infrastructure for data scientists and stakeholders to get access to the raw data themselves. This is related to the GET step above. The next layer involves using charts to display aggregate information. This layer is followed by reports. These layers are related to the steps in the Data Science Process, both ASK and EXPLORE. In charts, we produce visualizations to generate questions and problems. In reports, we are moving away from unstructured exploration to structured linking and reporting. The next layer is to add predictions. Here we are taking the previous layers and moving towards insights and recommendations for action. This fits well with the MODEL step of the Green Cloud process. The final layer is actions in which the previous layer leads to actual changes in organization strategy and tactics or is integrated into company products. There are a lot of different ways of characterizing the Data Science process as well as the evolution of Data Science teams and projects. For most of the text we will stick with the Green Cloud model except where the other models have an interesting perspective as we delve more deeply into the details. We’ll start here with the ASK step. 2.5 ASK So we start at the start. I assume that our organization has been collecting data as part of its regular business process and perhaps online presence. It might have done some marketing surveys. If not, there will be scant little for data scientists to do and they need to be hiring some data engineers. Let’s assume that “having data” is a solved problem for now…although our organization might not have all the right data. In the introduction to their Data Science uses cases, Kaggle provides a good set of starting questions for any data science project: What problem does it solve and for whom? How is it being solved today? How could a different solution (data driven) beneficially affect business? What are the data inputs and where do they come from? What are the outputs and how are they consumed (predictive model and a report? recommender and an email? statistical analysis and a dashboard?) Is this a revenue leakage (“save us money”) or a revenue growth (“makes us money”) problem? These are great prompts for the ASK step and are likely to lead to better conversations. Additionally, I would that the question or problem posed should be meet the general characteristics of a scientific question: is it falsifiable? For example, “Increase customer satisfaction” is not a falsifiable problem or question. Not only must have you have data but you also need to arrive at a metric with which to gauge success. However, we can elaborate on the ASK step a bit more by following the framework proposed by Max Shron Thinking with Data which also has a handy mnemonic, “CoNVO”. The goal of this framework is to make sure we pick an appropriate problem and set expectations about the solution appropriately. I like the CoNVO approach because it includes mock ups for the solution and thus plans for the final step in the Green Cloud, REPORT. It can also be used to track progress. Anytime you start to diverge from what was agreed upon during the “CoNVO”, it’s time for another “CoNVO”. That being said, I have only ever used this framework implicitly. I once brought up the idea of using it explicitly and got a bunch of blank stares. Apparently not everyone reads the same books I do! Don’t get overly hung up on details, though. This is a tool. 2.6 CoNVO As we stated previously, an interesting question is one that solves a business problem when answered. One way to make sure you have an interesting question is to follow the CoNVO framework. CoNVO stands for Context, Needs, Vision and Outcome. Context - What is the context of the need? Who are the stakeholders and other interested parties? Need - What is the organizational need which requires fixing with data? Vision - What is going to be required and what does success look like? Outcome - How will the result work itself back into the organization? You can see how the Kaggle questions fit into this as well, mostly during specifying the Need. For any Data Science project, you should know the CoNVO even if you don’t explicitly use those terms. Keep refining the CoNVO as you delve into the data. Let’s start with a specific example. In the book, Shron describes a situation faced by a university: Suppose that a university is interested in starting a pilot program to offer special assistance to incoming college students who are at risk of failing out of school (the context). What it needs is a way to identify who is at risk and find effective interventions (the need). We propose to create a predictive model of failure rates and to assist in the design of an experiment to test several interventions (the vision). If we can make a case for our ideas, the administration will pay for the experiments to test it; if they are successful, the program will be scaled up soon after (the outcome). So we have a context that includes the identification of a problem: some incoming college students are at risk of failing out of school and the university is interested in starting a pilot program. The need is to identify who is at risk and what interventions are effective. The vision is a proposal to create a predictive model of failure rates and to design experiments to test different interventions. The outcome is to communicate the findings and if they are robust, scale up the program. There’s a lot to talk about here. First, the context and need define an interesting question that solves a business problem (throughout this text, I equate “business” with both profit and non-profit). Second, the vision and outcome are a dry run of the blue steps get/explore/model and a proposal for the final communication step. You can think of the CoNVO as a dry run or meta process that happens before the actual Data Science process. With regard to the specific elements, most importantly, the need is not a dashboard or Hadoop or deep learning. A data product or tool is a potential solution; it is not a need. Shron notes, “A data science need is a problem that can be solved with knowledge, not a lack of a particular tool.” The solution is never “the CEO needs Tableau”. At Company Z, I built a dashboard from scratch that included logging so that I could see the most used features. That’s right, I went meta with my data science. When people said “we need this”, I could verify which features were and weren’t being used. If a feature wasn’t being used, I could ask, “why?”. It’s worth noting, however, that if your project has already been in progress for sometime, many of these questions will already be answered for you. While this might constrain your available options, you should feel free to investigate other possibilities. Shron’s approach of starting with a Need instead of the data is in stark opposition those who emphasize “playing with data”. As more and more data science projects fail, I feel like the “give data scientists data to explore and they’ll find you money” is becoming a caricature…or maybe not because I continue to hear of projects failing. In any case, this probably seemed more radical at the time. Because we start with a need (and not the solution) as we identify a vision (a potential solution) we are not constraining ourselves by the data. Of course, our vision may be too grandiose at the start, especially once we find out what data we actually have or what data we can actually afford. And this matches nicely with the Green Cloud process where we ASK the question or pose the problem and we imagine we have all the data we need. Then reality sets in and we have to GET the data we can. There’s a bit more to say about Vision. The vision is conveyed through Mockups and Argument Sketches. A mockup is a low level idealization of the final result of all our work usually something like a chart or charts, dashboard, spreadsheet, email or report. Mockups are common in the design of User Experience or User Interface in software and they can have the same function here. The Mockup like the wireframe of a website but for a data product or report. If you provide an example of the tables, statistics and chart that the report will include, you can get the stakeholder’s feedback on whether or not it is useful. Conversely, you can show someone a chart and ask them how it solves their need. Sometimes there will be a realization along the lines of “Oh, no, I just need this number”. Or you may realize that two charts looking at the data different ways are required. Some problems are solved by finding out a number, either its magnitude or sign (direction, negative or positive) to prove a point. An argument sketch is a template for settling that argument with placeholders for the actual values. Such an argument sketch helps you make sure you’re looking for the right number(s). This is sort of related to what I call “PowerPoint data science” but it is not limited to PowerPoint presentations. If someone claims that our churn rate is 3.2% then an argument sketch provides a template for the substantiation or refutation of that claim. Basically, before someone spends hours (or days or weeks) pulling data, we have agreed on what the actual dispute encompasses. Having a good mental library of examples (of mockups and argument sketches) is critical to coming up with a vision. This library of examples can be acquired by reading widely and experimenting. The importance of the mockup cannot be overemphasized. People grossly over estimate what they think they need to solve their problem and under estimate how much time it’ll take to solve it. A dashboard does sometimes fulfill a need but an email is easier, faster and cheaper. You can actually have too much information. If someone says, “I need a dashboard”…draw it on a whiteboard. Draw the charts. Ask them, what do these charts tell you and what decisions will they influence. You don’t have to be confrontational but now is the time to get clarity. Requests for dashboard are often preceded by the adjective “real time”. If it takes weeks to get your suppliers lined up, a real time dashboard is not needed…and can, in fact, be harmful. 2.7 CoNVO Examples In order to familiarize yourself with the framework, I have included a few examples here of CoNVOs from Shron’s book. 2.7.1 Refugee Non-Profit Context - A nonprofit reunites families that have been separated by conflict. It collects information from refugees in host countries. It visits refugee camps and works with informal networks in host countries. It has built a tool for helping refugees find each other. The decision makers are the CEO and CTO. So this is the context of the problem, where we find our interesting question. Need. The non-profit does not have a good way to measure success. It is prohibitively expensive to follow up with every individual to see if they have contacted their families. By knowing when individuals are doing well or poorly, the non-profit will be able to judge the effectiveness of changes to its strategy. This is the need, the actual problem, and what forms the basis for the interesting question. Vision. The non-profit trying to measure its successes will get an email of key performance metrics on a regular basis. The email will consist of graphs and automatically generated text. Mockup. After making a change to our marketing, we hit an enrollment goal this week that we’ve never hit before, but it isn’t being reflected in our success measures. Argument Sketch. “The nonprofit is doing well (poorly) because it has high (low) values for key performance indicators.” After seeing the key performance indicators, the reader will have a good sense of the state of the non-profit’s activites and will be able to make appropriate adjustments. This is how we’re going to solve the problem. We’re going to identify key performance metrics, generate graphs and descriptive text about them and send them in an email every week to the CEO and CTO. There are two suggestions for this vision. With the mockup we spec out an email that shows what it would be like for the system to generate an email that shows something the NGO has never seen before with its current record keeping, hitting an enrollment goal. With the argument sketch, we create a template for text that the system might generate. In either case, the stakeholders can imagine themselves obtaining this information and the insights it would contain and they can imagine making decisions based off of it. This is much more concrete with to the stakeholders with an example. Outcome. The metrics email for the nonprofit needs to be setup, verified and tweaked. The sysadmin at the nonprofit needs to be briefed on how to keep the email system running. The CTO and CEO need to be trained on how to read the metrics emails, which will consist of a document writtent to explain it. It’s not enough to generate an email. The result needs to be a data product with technical support, education and training on how to interpret the charts and text. There is no such thing as “intuitive” use. Using this discussion as an example, go through the following three examples and see how the CoNVO is documented. 2.7.2 Marketing Department Context. A department in a large company handles marketing for a large shoe manufacturer with an online presence. The department’s goal is to convince new customers to try its shoes and to convince existing customers to return again. The final decision maker is the VP of marketing. Need. The marketing department does not have a smart way to select cities to advertise in. Right now it selects targets based on intuition but it thinks there is a better way. With a better way of selecting cities, the department expects sales to go up. Vision. The marketing department will get a spreadsheet that can be dropped into the existing workflow. It will fill in some of the characteristics of a city and the spreadsheet will indicate what the estimated value would be. Mockup - By inputting gender, age skew and performance results for 20 cities, an estimated return on investment is placed next to each potential new market. Austin, Texas is a good place to target based on gender, age skew, performance in similar cities and its total market size. Argument Sketch - “The department should focus on city X because it is most likely to bring in high value.” The definition of high value that we use is substantiated for the following reasons. Outcome. The marketing team needs to be trained in using the model (or software) in order to have it guide their decisions, and the success of the model needs to be gauged in its effects on sales. If the result ends up being a report instead, it will be delivered to the VP of Marketing, who will decide based on the recommendations of the report which cities will be targeted and relay the instructions to the staff. To make sure everything is clear, there will be a follow-up meeting two weeks and then two months after the delivery. 2.7.3 Media Organization Context. This news organization produces stories and editorials for a wide audience. It makes money through advertising and through premium subscriptions to its content. The main decision maker for this project is the head of online business. Need. The media organization does not have the right way to define an engaged reader. The standard web metric of unique daily users doesn’t really capture what it means to be a reader of an online newspaper. When it comes to optimizing revenue, growth and promoting subscriptions, 30 different people visiting on 30 different days means something different than 1 person visiting for 30 days in a row. What is the right way to measure engagement that respects these goals? Vision. The media organization trying to define user engagement will get a report outlining why a particular user engagement metric is the best one, with supporting examples, models that connect that metric to revenue, growth and subscriptions; and a comparison against other metrics. Mockup. Users who score highly on engagement metric A are more likely to be readers at one, three and six months than users who score highly on engagement metrics B or C. Engagement metric A is also more correlated with lifetime value than other metrics. Argument Sketch. The media organization should use this particular engagement metric going forward because it is predictive of other valuable outcomes. Outcome. The report going to the media organization about engagement metrics will go to the head of online business. If she signs off on its findings, the selected user engagement metric will be incorporated by the business analysts into the performance metrics across the entire organization. Funding for existing and future initiatives will be based in part on how they affect the new engagement metric. A follow-up study will be conducted in six months to verify that the new metric is successfully predicting revenue. compare that to this: We will create a logistic regression of web log data using SAS to find patterns in reader behavior. We will predict the probability that someone comes back after visiting the site once. 2.7.4 Advocacy Group Context. This advocacy group specializes in ferreting out and publicizing corruption in politics. It is a small operation with several staff members who serve multiple roles. They are working with a software development team to improve their technology for tracking evidence of corrupt politicians. Need. The advocacy group doesn’t have a good way to automatically collect and collate media mentions of politicians. With an automated system for collecting media attention, it will spend less time and money keeping up with the news and more time writing it. Vision. The developers working on the corruption project will get a piece of software that takes in feeds of media sources and rates the chances that a particular politician is being talked about. The staff will set a list of names and affiliations to watch for. The results will be fed into a database, which will feed a dashboard and email alert system. Mockup. A typical alert is that politician X, who was identified based on campaign contributions as a target to watch, has suddenly shown up on 10 news talk shows. Argument sketch. We have correctly kept tabs on politicians of interest, and so the people running the anti-corruption project can trust this service to do the work of following names for them. Outcome. The media mention finder needs to be integrated with the existing mention database. The staff needs to be trained to use the dashboard. The IT person needs to be informed of the existence of the tool and taught how to maintain it. Periodic updates to the system will be needed in order to keep it correctly parsing new sources, as bugs are uncovered. The developers who are doing the integration will be in charge of that. Three months after the delivery, we will follow up to check on how well the system is working. You should not skip this planning under any circumstances but you may find resistance to such an overt approach (I have). However, you need to attach your efforts to real organizational needs and you need to constantly communicate progress with stakeholders. You may encounter some resistance if you’re working with people who thought data science was going to turn their pile o’ data into a pile o’ gold. But even implicitly adhering to this framework will guide expectations. 2.8 Resistance Resistance is futile. – The Borg Resistance is not futile. – Jean Luc Picard I wish it were all beer and skittles but that would be a lie. When you work as a data scientist or just do data science you will sometimes be picking around the organization’s sacred cows. There are going to be problems. And sometimes the problem is going to be you. People say they want data science but don’t want to give up control. People used to calling the shots may feel threatened. People may not cooperate with you. There may be a lot of folksy wisdom in the organization. Regardless of how glamorous you think your job is, however, you are not the most important function in the organization. People say they want data science, they want to be data-driven, they want data scientists…but they may be unwilling to give up the control that entails. Currently, most organizations operate by a simple rule: the highest paid person in the room decides. That doesn’t mean that options aren’t generated by everyone else but ultimately the highest paid person in the room has the responsibility. Or they operate by passive-aggressive consent. The chair/manager suggests something, everyone is intimidated into following along and when it goes wrong, the “team” is responsible. Problems with “group think” not withstanding, this view is very often in conflict with being data-driven. I have had several instances of this in my career so far. At Company Y, we were constantly offering promo codes in order to boost sales. Yet the data science team had proven that promo codes only shifted revenue. People didn’t buy more, they bought sooner and generally things they were already going to buy from us anyway. They just ended up buying them at a discount. At Company X, I was hired to do more sophisticated modeling that would be ground breaking for the industry. However, one of the managers wanted to just hack something together what everyone else had already been doing. The manager won. People you work with may feel threatened. As an embedded function, working with stakeholders and decision makers, you are going to run into some resistance from people who are not used to making decisions this way or doing things differently or just don’t want to change their workflow. One major obstacle is that we reward people for their seeming contribution. It is well known that the vast majority of stock brokers over the long run do not do better than the market. And yet stockbrokers who beat the market in any given year are given bonuses. They are feted. They are slapped on the back. Of course, it’s unlikely they’ll repeat that performance next year. But a year’s a long time and in 365 days, someone else will beat the averages and be crowned victorious instead. A marketing person may feel slighted that they are not picking the blue button that wins the company millions of dollars (and if it didn’t, at least they tried, right? Maybe next time!). The same is going to be true of designers who work hours on features, colors schemes, etc and then you mention “A/B testing”. What about their artistic vision? None of these people is wrong but there may have to be changes in what is valued in the company. The creative contribution in this case is to generate options (whether the button is blue or green) and not necessarily picking the winning option. And, honestly, there are cases where A/B testing isn’t appropriate. That doesn’t mean you shouldn’t collect data on the user experience to see which features are actually being used and how. People say they want data science but are not actually willing to be involved. Whether taking to you never reaches priority 1 or there’s just always something more important, it’s very easy, when dealing with a process that may take months, to put off participation. Don’t let this happen. Be determined. You need to talk to these people not only to make sure that your efforts are really solving their problems (and make sure you are solving their problems) but also because they are the source of domain knowledge you need to do your job. People are going to have beliefs about various aspects of the organization’s operations. “Everybody knows that our biggest donors are older women whose husbands have passed away.” When the data shows that your non-profit is anomaly, you need to be a bit tactful in revising the prevailing “folksy wisdom”. Unless you are an actual data science consultancy, you are overhead. You do not make the goods and services. You do not sell the goods and services. You are overhead. You may improve the production of goods and services. You may improve the sales of goods and services. Remember this. You are not the most important function in your business. 2.9 Conclusion The Data Science Process is the general context in which the various data science skills are executed. We start with ASK and find a question or problem that solves business needs. With GET, we obtain data suitable for answering the question or solving the problem. We EXPLORE the data to make sure the data is what we expected and to get a general sense of the data. Next we MODEL the data maybe using deep learning or maybe using something as simple as an average. Finally, we REPORT on our findings to that they can help stakeholders make informed decisions (or they find their way into our products). Data science teams and projects mature over time. We start with the basics (basic data, basic questions, basic models) often with only a few team members and poor data infrastructure. We look for breadth at the start because the first model is often the one that gives the biggest ROI. Even in a mature data science team, new areas may start out here. The main difference is that you’ll have some experience with the various growing pains. When I worked at Company X, we spent nearly two years working on recommendations for email. After those two years, the process was fairly well established and ironed out. But in the beginning there would be bugs and constant questions like “Why is my email this way?”. When we started personalizing the website, we went through the entire process all over again. “Why is the website this way?” We kept having to remind people, “we went through this with email, remember?”. These maturation steps are covered both by the Stages of Data Science and Agile Data Science Pyramid. However, because a project stops at using averages and rates and never moves to logistic regression, decision trees or deep learning, you shouldn’t think it was immature. Project should always use the appropriate technique. Determining the problem to solve and the technique to use can be determined by consulting stakeholders during the ASK step. Although Kaggle suggested a good set of questions to start with at the ASK step, we used Shron’s CoNVO framework to further flesh out the kinds of things we should discuss with stakeholders and plan up front. CoNVO stands for Context, Need, Vision, and Outcome. In talking to stakeholders and identifying a problem, we also identify the contexts of the problem and the actual need that is to be satisfied. We establish a vision for the solution that includes mockups and argument sketches and establish the outcome we want from solving the problem or answering the question. This is probably the “softest” chapter in the text but arguably the most important. No matter how good your technical skills, if you cannot identify a good and appropriate problem facing your organization and apply an appropriate solution and communicate your results, data science initiatives will constantly experience “existential threats”. 2.10 Review What is the Green Cloud model of the data science process? What four factors may influence the stages and maturity of a data science team or project? What are the Booz Allen Hamilton Stages of Data Science? What are the layers in the Agile Data Science Pyramid? How do the Stages of Data Science and the Agile Data Science Pyramid relate to the Green Cloud model? What does Shron’s CoNVO stand for and how is it used? Where does it fit into the Green Cloud? 2.11 Exercises Reverse engineer the application of the Green Cloud to one of the case studies from the previous chapter or one you have found. Reverse engineer the CoNVO for two of the case studies in the previous chapter or ones that you have found. Comparing the various case studies, can you guess at the various organizations’ data science maturity? 2.12 Additional Resources Elena Grewal - One Data Science Job Doesn’t Fit All (blogpost) Ganes Kesari - What are the 3 Stages where Data Science Teams Fail? (blogpost) Booz Allen Hamilton - The Field Guide to Data Science (website, PDF) Russell Journey - Agile Data Science (amazon) "],
["systems.html", "Chapter 3 Systems Theory 3.1 System Dynamics 3.2 What is a System? 3.3 Causal Loop Diagram 3.4 Heroin Model 3.5 Email Model 3.6 Conclusion 3.7 Review 3.8 Exercises 3.9 Additional Resources", " Chapter 3 Systems Theory In this chapter we discuss a bit of Systems Theory. Systems Theory? Systems Theory is basically an interdisciplinary field that seeks to understand the general properties of systems of interrelated parts and their interactions over time or “process”. In the 1970s and 80s, there was a huge interest in things holistic. In 1972, the Club of Rome’s Limits to Growth was published, which used Jay Forrester’s stock-and-flow modeling technique called System Dynamics. In 1975, Fritjof Capra published The Tao of Physics. By 1987, James Gleick had written Chaos: The Making of a New Science, which focused on the non-linear dynamics of systems. What all of these works have in common is a desire to make sense of systems as a whole based on the behaviors of their interdependent parts. Chaos Theory emphasized the non-linear and often unpredictable nature of such systems. Systems Theory can be informative for data science for a number of reasons: Domain knowledge is an important aspect of data science. The data do not “speak for themselves”. Systems do exhibit general properties such as “increasing returns” or “decreasing returns” and it is important to be able to identify when the system you are working with is exhibiting one of these properties. The tools of Systems Thinking can be used to refine a collective understanding of Context and Vision (CoNVO). In this chapter we discuss Systems Theory, specifically, a qualitative modeling technique called “Causal Loop Diagrams” that originated with System Dynamics. We focus on the Causal Loop Diagram (CLD) because: A Causal Loop Diagram enables you to extract domain knowledge from stakeholders and interested parties (Context). All data comes from some real world process. A Causal Loop Diagram is a qualitative way of making your thinking about that process concrete (Context). A Causal Loop Diagram then helps you develop suitable quantitative models (Need). In my experience, the first weeks and sometimes months of a new job or orienting a new team member involves figuring out the domain vocabulary, data locations and availability as well as basic goals of the organization. In other words, you acquire domain knowledge. Now, while I cannot teach you the domain knowledge you need for your particular job, it turns out there are some well understood principles that all domains more or less follow. I find there are some general techniques for thinking about problems that fit very well with Data Science and are worth knowing. Once such technique is Systems Theory or, more specifically, Systems Dynamics. Systems Dynamics covers a wide range of modeling techniques. I am going to go over only one and I’m not even going to suggest that you follow it exactly. The main goal is to make your thinking about the domain concrete and be able to share it with others. This is really helpful for rooting out stakeholders with differing mental models of what’s happening or what needs to be done. Otherwise, there is a tendency for everyone to believe that they’re talking about the same thing or leave everything at vague generalities. But probably most importantly for us, it turns out this technique is a good basis for talking about what our data is and where it comes from (the data generating process) and ultimately what your models are about (and, in some cases, how they might affect the real world). 3.1 System Dynamics It turns out that we, homo sapiens sapiens, aren’t particularly good at understanding relationships involving more than a handful of variables with more than one or two degrees of separation. We get simple cause and effect but when we start adding more variables and feedback loops, we get lost quickly. System Dynamics was originally developed by Jay Forrester as a technique to help managers better understand the system they were managing. The basic approach is to identify key variables of the system and their influences on each other and model them with a set of differential equations. More specifically we look at levels (population, industrial capacity, atmospheric carbon) and flows (births, deaths, new products, output). Given the parameterization of the model, the point is not to see if Gross Domestic Product is estimated to be $500 trillion or $600 trillion next year. The point is to see the pattern in the behavior of GDP, whether it goes up or down or whether it oscillates. The relative values of the parameters can often produce widely different outcomes, even for the same model structure. And this is the interesting thing about System Dynamics. It basically says something like, “I don’t care if you’re talking about an economy or a cell in the body or a corporation. There are general systematic patterns at work when two flows are working against each other.”. Today, there are software packages available to develop these models graphically and then simulate them. The first use of System Dynamics (all computations by hand!) was probably Forrester’s consulting work with GM to explain a 3 year cycle in employment at a Kentucky GM plant. The most famous application of System Dynamics was the Club of Rome’s Limits to Growth report published in 1972. Limits to Growth criticized when it first came out but the general path of the predictions for “business as usual” appear to be on track. Unfortunately, this particular scenario predicts economic collapse around 2050 (give or take). Systems Dynamics course is still taught at MIT by Forrester’s students and their students. There’s a lot more to System Dynamics than than what we’ll discuss here. For example, there are archetypal patterns in complex systems. Additionally, Donella Meadows identified 12 leverage points in a system. My personal view is that System Dynamics contains many of the most useful insights of economics (for example, decreasing returns) and more. While Stock and Flow Models are interesting, the more qualitative aspects of Systems Dynamics are sufficient for what we want to accomplish: flesh out the relationships between the variables in our systems and create a shared understanding. For example, if you’re trying to investigate one of the Kaggle data science use cases, say, training recommendations in an HR context or monitoring contractor performance in construction, then we want to be able to quickly sketch out the main relationships of this system in order to develop the context, need and vision (CoNVO). For this purpose, I think that System Dynamics’ Causal Loop Diagrams (CLD) are a useful tool. They can be used during the early stages of context and need identification with domain and subject matter experts, stakeholders, and other analysts. They can improve communication, by establishing a shared mental model of the system being evaluated or manipulated, and bring to light unknown factors and hidden assumptions. And, perhaps more importantly, they can be used even if it’s just you. As we will see later, understanding the system is imperative to all steps in the data science process. CLDs can help in a number of ways: It represents a shared understanding of the domain knowledge. It represents a mental or qualitative model of where the data is coming from. It represents parts of the system that may have been “assumed away” or “simplified”. It represents a broader context for evaluating the success or failure of a data science solution. First, it’s one thing for people to say “sending more email creates more sales” and another thing to start actually specify a system where that’s true. CLDs give you a notation for describing a system and its parts as well as the influences of each part. It also provides a broader context for data-based story telling. Second, data is not just columns of numbers and symbols. Data is generated as observations (direct or indirect) of a system in motion, a process. Taking the time to actually specify the system that is generating the data will help you understand the data better. Third, when we build statistical or machine learning models, we often do not have all the information we would like (either because we didn’t get it or we can’t get it) or because we chose not to include it. With a CLD, we have a baseline of comparison for what we did not, could not, or would not include. At the data collection stage, this can be a warning to us that something doesn’t seem right (“why are there values for this variable above 10?”) or at the modeling stage, that something may be wrong or unexpected (“we thought this relationship would be positive and the coefficient is negative. We need to check the data.”). Fourth, and this relates to the second, it may turn out that an email campaign that tested well, flops utterly when scaled. Why? We often assume the world is unchanging or stationary when it is not. Perhaps a competitor got wind of the email campaign and launched their own advertising blitz that neutralized it. Finally, when talking about systems it’s convenient to have a notation for describing the system and that’s where Causal Loop Diagrams come in. But first let’s start at the beginning. What is a system? 3.2 What is a System? What is a System? Good question. At this point we simply defer to Wikipedia: Some systems share common characteristics, including: A system has structure, it contains parts (or components) that are directly or indirectly related to each other; A system has behavior, it exhibits processes that fulfill its function or purpose; A system has interconnectivity: the parts and processes are connected by structural and/or behavioral relationships; A system’s structure and behavior may be decomposed via subsystems and sub-processes to elementary parts and process steps; A system has behavior that, in relativity to its surroundings, may be categorized as both fast and strong. The term system may also refer to a set of rules that governs structure and/or behavior. Alternatively, and usually in the context of complex social systems, the term institution is used to describe the set of rules that govern structure and/or behavior. Systems are often part of larger systems (and systems often have smaller parts that are themselves systems). A cell is a system. The digestion of food is handled by a system (even though it includes cells). A person is a system (even though it has a digestive system). A family is a system (even though it includes persons). And so on. As Carl Sagan said, “if you want to bake an apple pie from scratch, you must first create the universe.” Where you draw the line depends on what level of behavior you’re interested in (we’ll have a bit more to say about this later). I recommend going out one or two layers just beyond the immediate interest or control. If you stop at what you’re interested in or what you can directly control, you may well leave out important parts of the system and their interactions. Going slightly “bigger” or “wider” also helps you be more certain that your simplifications are either warranted or document exactly what was assumed away. 3.3 Causal Loop Diagram A Causal Loop Diagram is, at its most basic, a directed graph. Unlike such constructs such as Bayesian Networks, a CLD may have cycles. In fact, the presence of cycles is what makes System Dynamics unique. Variables are indicated by nodes or vertices as in the first graph of Figure 3.1. In Figure 1, both A and B are variables. In the next graph, we see that A influences B. Arcs indicate influence between two variables with the arrow indicating the direction of influence. All arcs have a polarity notation near the arrowhead indicating “positive” (+) or “negative” (-) polarity. Polarity is established by mentally walking through the (causal) influence of A on B. When A increases, what happens to B? If B also increases, then the polarity is positive. It’s very easy to misread a CLD and believe that the plus sign (+) indicates “increase”, that B increases no matter what happens to A. Figure 3.1: Two Variables with Positive Polarity Looking at Figure 3.2, we see what happens when A decreases, B also decreases. Positive polarity indicates that two variables move in the same direction. Actually, in the “old days”, polarity was indicated by “S” (for same) and “O” (for opposite). Figure 3.2: Positive Polarity Works Both Ways In Figure 3.3, we establish that a different relationship between A and B. if A increases, B decreases. This is the negative polarity as indicated by the negative sign near the arrowhead. Figure 3.3: Negative Polarity Indicates an Opposite Reaction In Figure 3.4 we can see that A can influence B and, simultaneously, B can influence A. Such cycles–called feedback loops–are an important part of System Dynamics. The (+) in the middle indicates that this is a reinforcing feedback loop (which we will also indicate with (R)). In such a feedback loop, the interactions of the variables reinforce each other: A increases which causes B to increase which causes A to increase and so on. Figure 3.4: Population Model If there are arcs in the feedback loop with negative polarities, then we may have a balancing feedback loop (- or B) as in Figure 3.5. This is fairly easy to figure out with only two arcs in the loop, but how does it work with many arcs? The rule of thumb is that if there are an odd number of negative arcs in the loop, then the feedback loop is balancing; otherwise, it is reinforcing. Figure 3.5: Balancing Feedback Loop Let’s see a simple example of CLD modeling using subject matter with which everyone is familiar: life and death. In Figure 6, we see a simple population model with three variables: births, populations, and deaths. There is a positive arc from births to population because when births increase, population increases. There is a positive arc from population to births because when population increases, births increases. However, they do not increase right away but with a delay. This delay is indicated by the two strikes on the arc, “||”. Because there are no negative arcs (an “even” number), this is a reinforcing feedback loop as highlighted in the 2nd graph of Figure 6. Similarly, as population increases, deaths increase and so that arc has a positive polarity. Because members of the population do not die instantly, there is a delay on the arc. However, as deaths increase, population decreases and so that arc is negative. Because there is one odd negative arc, this is a balancing feedback loop (3rd graph of Figure 6). Figure 3.6: Simple Population Model Although this is only a simple model, we can see a few interesting things. First, population itself is kept in check by the relative strengths of the two feedback loops. Second, from a modeling perspective, there are a number of parameters that are exogenous to the system: puberty, birth rates, lifespans, and death rates. However, we can endogenize a number of these variables by including them as factors in our diagrams. In Figure 3.7, we add at least one perspective on birth rates. Starting with population, when population increases, resources per person decrease (negative polarity). When resources per person increase, life expectancy increases (positive polarity). When life expectancy increases, the desired family size decreases (negative polarity). When desired family size increases, births per person increase (positive polarity). It is very important to look at pairwise relationships. At this point, we’re not following the causal chain yet. There is a natural tendency to want to “run” the model while you build it. For example, we could have taken the “desired family size decreases” and applied it to “births per person” but we would have almost certainly confused ourselves. It’s easiest to simply take each relationship in isolation and start, “When this variable increases, what does that variable do?”. When births per person increases, then births increase (positive polarity). Finally, when births increase, the population increases (positive polarity). Because we have two negative polarity arcs in the loop, this is a reinforcing feedback loop. Figure 3.7: Endogenized Birth Rate Now, let’s mentally simulate the model. When population increases, resources per person decrease. This causes life expectancies to decrease. As life expectancies decrease, desired family sizes increase. As desired family sizes increase, there are more births per person. As births per person increase, overall births increase. This increases population (which decreases resources per person…). In this way, we have endogenized birth rates (although not puberty). Figure 3.8: Simulating the Expanded Birth Cycle Next we look at death rates in Figure 3.9. This is a lot simpler. As life expectancy increases, deaths decrease (negative polarity). With three negative polarity arcs, the entire loop is a balancing feedback loop. Figure 3.9: Endogenized Death Rate Finally, you can follow the entire cycle in Figure 3.10. Figure 3.10: Simulating the Expanded Death Cycle Note how we started simple and then expanded the model by endogenizing the birth rate and the death rate. Starting simple is a good way to track your progress. Also note that the characteristics of the feedback loops did not change. If, after you expand a model, the feedback loop flips from reinforcing to balancing (or vice versa), you should re-evaluate what you’ve done: was the initial model wrong because it left out details or is there something wrong with the new model. In general, however, this is typically how you’ll proceed. As a model is developed, what was an exogenous parameter can become an endogenous variable. You should not get bogged down in the process, however. This is only a tool. My suggestion is that if you are starting with the problem (either alone or on a team), start creating a causal loop diagram so that you can all clarify your mental model of the problem, the needs, vision and outcome. If you are starting with the data, go back and try to build the CLD that gives rise to the data you have even if it means adding variables that you do not have have data for. This will clarify your thinking about the data you have and the relationships you expect to see. I should also note that proper CLDs only have numerical variables or features. You will not generally see a variable labeled “gender” or “political party”. For example, men and women have different life expectancies. I think it’s fine to include these categorical variables in a Causal Loop Diagram either as variables or parameter. However, if you can think of a numeric representation of the variable, “social conservatism” instead of political party, you should use it if that is what you really mean (it’s possible that “political party” would then be a proxy for social conservatism in your actual model). Another approach is to change a categorical variable into a multiple binary variables. Here we could change political party to Democrat and think of the variable as a switch. If we switch it on, what happens to the other variable? If another variable increases, does the Democrat variable get switched on or off? The solutions are imperfect but it can be made to work. Let’s walk through a few more examples in order to get feel for the approach. First, let’s take a look at the famous “Heroin Model” and see why CLD can be very important for Data Science. 3.4 Heroin Model This is a fairly famous application of System Dynamics to a public policy problem. It dates from the 1970s but is topical with the resurgence in heroin use as a result of the opioid crisis. There are a few things I’d quibble with both in the Original and Revised Models but let’s accept them for now. Figure 11 shows the original model for the heroin cycle in the Bay Area of California (I think it was Berkeley or Oakland). Let’s walk through the three main loops of the model starting with “heroin supply”, first looking at the pairwise relationships in the arcs between variables. When the heroin supply increases, crime increases (positive). When crime increases, police activity increases (positive). When police activity increases, heroin seizures increase (positive). When heroin seizures increase, the heroin supply decreases (negative). The entire loop is balancing because there is a single negative arc. The next loop also starts from heroin supply. As the heroin supply increases, the heroin trade increases (positive). As the heroin trade increases, police activity increases (positive). We’ve already seen the rest here. Because there is only one negative arc, the entire loop is balancing. Finally, the third loop also builds off the heroin supply. As the heroin supply increases, the number of dealers increases (positive). As the number of dealers increase, the number of addicts increases (positive). As the number of addicts increases, the heroin trade increases (positive). This third loop is also balancing because there’s only the single negative arc between seizures and heroin supply. Because all three loops are balancing, the entire system is self-correcting. In essence, if there is no heroin supply, there won’t be dealers, addicts, or crime. At the time, police activity was identified as a leverage point in the system because police activity could be changed independent of the rest of the system (through extra patrols, tougher enforcement, etc.) and it could directly affect seizures and thus the heroin supply. This became the policy. Both crime and the heroin trade increased, instead of decreasing. What went wrong? Figure 3.11: Original Heroin Model The main problem was a failure to identify a key variable of the system: the price of heroin. As Figure 12 shows, an increase in the supply of heroin decreases the price. But, more relevant to the problem, a decrease in the supply of heroin increases the price. And what does policy activity do? With the increase in price, the heroin trade increases (more dealers are attracted). It also directly influences criminal activity because the price per addict is higher. This leads to more burglaries and theft. The main failure here is to recognize how price and price elasticity would influence the market and trade for heroin. Increased police activity actually made things worse because all the loops are actually reinforcing loops. Police activity wasn’t the leverage point we thought it was. Figure 3.12: Revised Heroin Model Does this mean that System Thinking failed? No. The main lesson here is that you might and will get your model wrong but by having it written down, we can at least see what each other is thinking. 3.5 Email Model This scenario is not so much about a failed model as a failure to model. At Company X, we made our living, more or less, by sending emails to our subscribers. Figure 13 shows the simplest possible model for this relationship. Figure 3.13: Simplistic Email Model The problem with this model is that it is much too simple. If this is your mental model, you can use it to achieve any target revenue revenue you’d like by simply dividing the revenue target by revenue per email and sending that many emails. This model entirely ignores the process in which emails and subscribers exist and revenue arises. Would the stakeholders have noticed how simplistic their model was if we had written it down just like this? An interesting question. So how can we make the model more realistic. First, we recognize that we’re sending emails to a list of subscribers. Second, we recognize that whenever you apply an action to a fixed resource (the list of subscribers), you can experience constant, increasing, or decreasing returns. Here domain knowledge from the Marketing department says that we experience diminishing returns. Why? Figure 14 shows a more in-depth model of the email/purchase process. We start with subscribers. If we increase subscribers, we can increase the number of emails. If we increase the number of emails, the number of unsubs increases. An increase in unsubs will decrease the number of subscribers. This is an important feedback loop that was missing in the original model. We are also constrained by inventory. If we have one item in our inventory, we can’t very well send a single person 20 emails about it. An increase in inventory increases the number of emails we can send. An email isn’t a thing by itself. An email is sent at a specific time, with a subject line and content. All of these influence whether an email leads to a purchase or not. An increase in emails, increases the number of emails that are opened. An increase in the number of opens leads to an increase in clicks. An increase in clicks leads to an increase in purchases. Finally, we can influence some of these things using personalization. If personalization “increases” (gets better) then opens increase (because they have better subject lines) and clicks increase (because they have better content). With an increase in inventory, we get an increase in personalization (we have more things to choose from to show you). An increase in purchases permits us to personalize better. This concept is called “revealed preferences” (you tell us the kinds of things you like by buying them). Figure 3.14: Realistic Email Model Now, what does this say about sending emails? If we use emails as a leverage point (if we just change–increase–the number of emails we send), then subscribers goes down. We also see that opens, clicks, and purchases go up but here we have the diminishing returns. The increase in purchases at the one end will eventually be offset by the loss of subscribers. This new model isn’t perfect. For example, I don’t believe it captures all aspects of diminishing returns. You can’t send someone 20 emails a day about the same five things. You probably can’t send 20 emails a day about five completely different things. It isn’t clear to me how to capture that. But at least the limitations are made clear in the discussion. Additionally, we can get new subscribers through marketing and buying email lists. The interesting challenge here is that there are probably diminishing returns with new subscribers as well. Given the typical adoption curve and your place on it, the most enthusiastic customers for your product have already joined. This means that if you are adding truly new customers, they are going to be late adopters and perhaps harder to convince to buy your products. Nevertheless, the model is a start. By modeling the different parts of the email/purchase process, we can start measuring them and monitoring the health of the system as well as improve it. It serves as a foundation for communication between Finance, Marketing, and Data Science. 3.6 Conclusion Data Science is using math and computers to solve a problem that stems from a lack of knowledge. We can frame the application of data science in terms of the context, need, vision and outcome (CoNVO). However, for any particular application of data science, we need to have domain knowledge. One way to frame that domain knowledge is in terms of understanding the system in which we are operating. If our CoNVO leads us to build a model of at-risk youth, we should understand the system in which this process is taking place. Systems Thinking is general approach to think about the processes as interrelated, changing parts. System Dynamics is a particular approach to Systems Thinking. Stock and Flow Diagrams and Causal Loop Diagrams are models that System Dynamics uses to talk about systems and their evolution. Causal Loop Diagrams are a very useful tool for framing all aspects of the data science process. They allow the team and stakeholders to share their mental models of the process being studied or acted upon. They help us make sure we’re just simplifying and not being simplistic. They help identify data that we need or what we should model. They keep us honest about the degree to which we can really intervene in a particular system or how our interventions are likely to fail. One of the most famous examples the non-linearity of systems comes from the 1970s. In San Francisco (IIRR), the police started cracking down on heroin dealers with the hope of lowering drug abuse. Instead, burglaries and other petty crimes increased. Why? Cracking down on heroin dealers caused the price of heroin to increase. This increased price caused theft to rise because addicts needed more money to pay for the increased price of heroin. “hi-tech” modeling techniques like boosted random forests and deep learning do not excuse the data scientist from understanding the system they’re modeling or hoping to control. 3.7 Review Why is domain knowledge controversial in some data science circles? Why are Causal Loop Diagrams helpful for data science? How do Causal Loop Diagrams fit into our overall “CoNVO” model for describing successful data science projects? The main components of a Causal Loop Diagram are: variables, influence arrows and polarity arrows. If there is an arrow from variable A to variable B with a “+” sign at the arrow head, what does that mean? If there is an arrow from variable B to variable A with a “-” sign at the arrow head, what does that mean? Taken together, what kind of feedback loop does the above arrangement describe? If, instead, the arrow from variable B to variable A had a “+” annotation, what kind of feedback loop would that describe? What are the difficulties associated with categorical variables and Causal Loop Diagrams? What are some of the ways we can get around them? 3.8 Exercises Add the potential for getting new subscribers to the email model. How do you add the adoption curve to the model? Find a problem from work or that interests you and see if you can draw a Causal Loop Diagram for it. Share it with someone who has an opposing viewpoint. Share it with someone who has the same viewpoint. What assumptions do you have in common? Do you disagree on the polarity of any arcs? Is there “violent agreement” on anything? 3.9 Additional Resources Limits to Growth was Right: New research shows we’re nearing economic collapse (news) Systems &amp; Us (site) Overview of Causal Loop Diagrams (video) Brief Overview of Causal Loop Diagrams (video) Causal Loop Diagrams (course materials, pdf) Vensim (software) The Evolution of Systems Thinking (slides) "],
["probability.html", "Chapter 4 Probability 4.1 From Systems Thinking to Probability 4.2 Definition of Probability 4.3 Axioms of Probability 4.4 Types of Probability 4.5 Rules of Probability 4.6 Independence and Conditional Independence 4.7 Probabilistic Fallacies 4.8 Applications 4.9 Probability and Simulation 4.10 Conclusion", " Chapter 4 Probability In this chapter we will discuss the probability as a framework for working with uncertainty. We’ll start by defining probability then move to the axioms of probability. We’ll talk about different types of probability that cover scenarios for complex events when some characteristics of the outcome are known. We’ll then move on to rules of probability that permit us to manipulate probabilities. Probability is used to talk about the plausibility of events. Sometimes the co-occurrence of events gives us additional information (if someone one is wealthy, they may also wear designer labels). However, sometimes the co-occurrence gives us no additional information. We will talk about independence and conditional independence. Probability is tricky and associated with a number of fallacies, which we will discuss. This forms a subset of fallacies associated with data analysis in general which we will cover in a later chapter. Finally, we’ll go over some applications of probability and the role of simulation in solving problems involving probability. 4.1 From Systems Thinking to Probability It may not be intuitive but the use of probability as a tool to model plausibility arises naturally out of systems thinking. To explain how, we consider the canonical example of probability: flipping a coin. If we think about a Coin Flipping System, what are the elements of this “flipping a coin” system? Obviously, the coin has something to do with it, maybe its shape, size, weight, balance, etc. Coins will behave differently on the Earth than on the Moon or elsewhere in the universe so the gravity, atmosphere, etc. all play a role. There’s the size and shape of the hand, location of the coin, and the force applied. Finally, there’s this universe’s general laws of physics. So “flipping a coin” is a system and, more specifically, it is a deterministic system. But it turns out that we cannot actually measure most of the variables that influence the outcome (whether the coin is heads or tails) and because of this, even though there is no actual variable called “chance”, it’s easier to model the variable “coin flip” as being random. The key point, though, is that it is not “arbitrary”. We’re using probability to model the system of flipping a coin because it’s a great way of working with uncertainty (the opposite of plausibility, we’ll use both terms). It is perhaps unfortunate that the theory of probability was developed around gambling because the whole idea of “games of chance” biases our views about what probability is and what it can be used for. The concept of “probability” thus becomes associated with some fuzzy ideas about “chance”, “randomness” and “arbitrariness”. Throughout this text, we just take probability to mean a measure of plausibility or uncertainty…and usually that uncertainty comes from ignorance. Not from some inherent, mysterious “randomness” in the process. On the other hand, we do have everyday notions and ideas about probability. We often say that things are “probable” or “likely” and we’ll talk about the “probability” of something happening or the “likelihood” of something happening. These actually fit very well with our idea of probability as measuring uncertainty. However, we need to bear in mind that although we will often use the colloquial “likelihood” now and again, “likelihood”, as we will see, has a technical meaning as well. In general, this should not be a problem as the context will tell us which meaning of likelihood is being used. It’s just something we need to be aware of. Long story short, when we flip a coin, and we don’t know about or we simply ignore all of those factors we cannot measure, we need a way of working with the resulting uncertainty over the outcomes. Probability allows us to do that. As a side note, if you don’t believe there isn’t at least one deterministic coin flipping system, researchers have built a coin flipping machine that can reliably make a coin always come up heads. 4.2 Definition of Probability Data is ultimately evidence for a claim. In Data Science, we want to use data to establish claims that answer a question or solve a problem. The degree to which data is supports a claim could be called the claim’s plausibilty. As we gather data, some claims will be more plausible than others. Our tool for dealing with plausibility is probability. Following Edwin Jaynes, we see probability as an extension of logic from claims that are true or false to claims that have different degrees of support. Consider the following syllogism in logic: If something is a person, it is mortal. Socrates is a person. Socrates is mortal. This is an example of modus ponens. Now consider the following syllogism: If something is a swan, it is white. This animal is a swan. This animal is white. Of course, when this syllogism was framed, Australia had not yet been discovered by Europeans. However, when the Europeans arrived, they found black swans. The syllogism is now false because the first statement is false. In logic, veracity is an all or nothing quality. Statements are true or false. While there are various methods of handling degrees of truth, we will focus in this text on probability. Probability permits us to work with statements like the following: If something is a swan, it is likely to be white. This animal is a swan. It is likely to be white. Probability enables us to work in a world where the evidence is not clear cut and only lends partial support–plausibility–to a claim. The evidence might also support multiple claims to varying degrees preventing us from being absolutely certain in our decisions. In this chapter we will discuss the basics of probability. In the next chapter, we will combine probability and systems thinking to talk about stochastic processes. In a later chapter, we will return to probability and talk about the continuous version of modus ponens, Bayes Rule and statistical inference. 4.3 Axioms of Probability There are different versions of the “axioms of probability” but we’ll use the Kolgomorov Axioms of Probability because they’re straightforward. They define the basic properties of a probability metric. Probability is defined over outcomes (we’ll talk about events later). Outcomes are, more or less, mutually exclusive observations about a fixed domain, which can generally include the properties of objects. We’ve already discussed the coin flip, W = {\"heads\", \"tails\"}. If we were talking about customers, we might have an outcome space of W = {\"purchase\", \"not purchase\"}. If a customer purchases something, we can talk about an outcome space of where they live, S = {\"AL\", \"AK\", \"AR\", ..., \"WY\"} and their gender, W = {\"male\", \"female\"} (very few businesses account for non-binary identification). The customer’s state of residence and gender are properties of the person. Finally, we can talk about joint outcome spaces as the Cartesian product of simple outcome spaces, S x G = {(\"AL\", \"male\"), (\"AL\", \"female\"), (\"AK\", \"male\"), (\"AK\", \"female\"),...}. We will define different types of probability later but for now we take P(W) to mean “the probability of”. The argument to P(W), W, is an outcome space, such as state or gender. P(W) can be thought of as a assignment of probability to each element of W (as a table of probability values or perhaps a function). P(W=\"male\") returns the probability of the single outcome in W, W=\"male\". Following the conventional notation, when confusion is unlikely, we can also write P(male) to mean the same as P(W=\"male\"). If we let \\(W\\) be the set of all possible outcomes and \\(w\\) be some particular outcome (which are considered to be atomic or mutually exclusive) then the axioms of probability are: \\(P(w) \\geq 0\\) \\(P(W) = \\sum_i P(w_i) = 1\\) \\(P(w_i \\cup w_j) = P(w_i) + P(w_j)\\) The first axiom states that a probability, a degree of certainty, must be non-negative. While one can certainly think of an interpretation of a negative probability (perhaps our certainty against an event happening), it is cleaner to think of all probabilities as being non-negative. The second axiom says that we must be 100% certain in all the outcomes in \\(W\\) taken together. At minimum, one of them must happen. It also constrains probabilities to be on the range \\((0, 1)\\). We can always convert an “improper” probability distribution into a proper probability distribution by normalizing: dividing all “improper” probabilities through by the sum total. In fact, we may often do this on purpose when eliciting probabilities from people. If we assign a certainty value of 1 to an outcome A, we can ask someone if they feel that outcome B is twice as likely to happen (in which case it would get a value of 2), or half as likely to happen (in which case it would get a 1/2). We can then go through and normalize. The third axiom says that the probability of \\(w_i\\) or \\(w_j\\) is equal to the sums of the their individual probabilities, \\(P(w_i)\\) or \\(P(w_j)\\). If plausibility in rain tomorrow is 0.23 and our degree of belief in snow tomorrow is 0.10 then our degree of belief–probability–in either rain or snow tomorrow must be 0.33. This axiom is known as the Additive Law of Probability. Note that this is only true because we’re talking about atomic outcomes. It is sometimes the case that the axioms are defined in terms of events. An event is a generalization of an outcome and may contain several outcomes. Rolling a 1 on a six-sided die is an outcome. Rolling an odd number on a six sided die is an event. Note that rolling a 1 on a six-sided die is also an event which can lead to some confusion. If we consider events, then the 3rd Axiom becomes: 3a. \\(P(E_i \\cup E_j) = P(E_i) + P(E_j) - P(E_i \\cap E_j)\\) Why the difference? The first version of the axiom is defined for outcomes which must be mutually exclusive. The second version is defined for events which may not be mutually exclusive. Events can be collections of outcomes. For example, \\(W\\) might be the sides of a six-sided die \\(W = {1, 2, 3, 4, 5, 6}\\). These are the possible outcomes. In contrast, \\(E_1\\) might be “all even valued sides of the die” and \\(E_2\\) might be “all sides whose value is &lt; 4” which are events. While the outcomes in \\(W\\) are mutually exclusive, the events in \\(E_1\\) and \\(E_2\\) are not (they have the value 2 in common). If follows that all outcomes are events but not vice versa. The power set of a set \\(X\\) is the set of sets generated by combining all possible elements of X into sets. For example, {1, 2, 3} has the power set [{1, 2, 3}, {1, 2}, {1, 3}, {2, 3}, {1}, {2}, {3}, {}]. You can think of events as coming from the power set over the set of all individual outcomes with only some of them being of any interest. If \\(W\\) defines the set of outcomes, then the power set defines all possible events. Note that all outcomes are events but not all events are outcomes. So the power set of {All US States} will include {AK, HI}, {AR}, {ME, NM}, {CA, AZ, OR, WA, NV}. Some of these may be of interest to us and some may not and some of these sets have names such as “Continental US” or “Western States”. The upshot is that all outcomes are by definition mutually exclusive but events are not. When working with events, we must use the modified versions of Axioms #2 and #3. We will usually refer to events and need only worry about whether or not they’re atomic or mutually exclusive. This meshes nicely with the language of software engineering in general and logging specifically. 4.3.1 Notation Probability notation can get a bit crazy. We’ve already mentioned that \\(P()\\) can mean a lot of different things: \\(P(A)\\) - is the probability distribution over the outcomes/events of A. It is most likely a table of events and probability values. \\(P(A=a_1)\\) - is the probability that A takes on the value a1. It’s a single probability value. \\(P(a_1)\\) - when the context is not ambiguous, this is a shorthand for the above. It is a single probability value. What if there’s more than one variable? \\(P(A, B)\\) - is the probability distribution over the Cartesian product of outcomes/events in A and B. If A = {a1, a2, a3} and B = {b1, b2} then \\(P(A, B)\\) returns a single probability value for each of (a1, b1), (a1, b2), (a2, b1), (a2, b2), (a3, b1), (a3, b2). \\(P(A|B)\\) - is actually multiple probability distributions. The \\(|B\\) part is read as “given B”. This is known as a conditional probability which we’ll talk about later. There is one probability distribution for each possible value of B. So if B = {b1, b2, b3, b4}, \\(P(A|B)\\) is actually four probability distributions. Operations on probability distributions are kind of like joins in database queries: \\(P(A)P(A)\\) - This is an outer join of A (cartesian product) with itself. \\(P(a1)\\) * \\(P(a1)\\), \\(P(a1)\\) * \\(P(a2)\\), \\(P(a2)\\) * \\(P(a1)\\), \\(P(a2)\\) * \\(P(a2)\\). \\(P(A)P(B)\\) - This is an outer join of A with B. \\(P(A|B)P(B)\\) - This an inner join. Remember that \\(P(A|B)\\) is actually multiple probability distributions. This means that if B = {b1, b2} then we have \\(P(A|B=b1)P(B=b1)\\), \\(P(A|B=b2)P(B=b2)\\). In this case, B acts as the “foreign key” between the two sets. As you work through some examples later, you’ll get a better feel for how it all hangs together. 4.4 Types of Probability When we have a complex event space, we may have different types of probability. These probability types cover joint probability, conditional probability, and marginal or prior probability. 4.4.1 Joint Probability Consider a social and economic process whereby certain areas are more or less densely populated, have more jobs or better jobs, or have different kinds of industries. We can think of these areas as having two properties, Community and Income. We may not have sufficient information to characterize this process (a system that changes over time) for a particular area and so we want to use probability to describe how plausible certain occurrences of these joint events are. The possible events for Community (C) are {urban, suburban, rural} and the possible events for Income are {low, high}. Taken together these define an event space that is the cross product of the two sets {(urban, low), (urban, high),…}. If this seems weird to you, it is! Normally we work the other way around. We work from data (events) about communities and infer these proportions. Inference is covered in a later chapter. I want you to get used to use probability for things other than card games. The following joint probability distribution over these two sets measures our belief that the next observed area will some combination of properties for Community and Income: area low high rural 0.04 0.02 suburban 0.19 0.22 urban 0.29 0.24 Each entry in the table is a particular probability estimate such as: \\(P(C=urban, I=high) = P(urban, high) = 0.24\\) \\(P(C=rural, I=low) = P(rural, low) = 0.04\\) If we want to know the probability of the event (urban, low) or (urban, high) then by axiom #3, we have: \\(P(urban, low \\cup urban, high) = P(urban, low) + P(urban, high) = 0.29 + 0.24 = 0.53\\) and similarly, if we want to know the probability of (urban, high) or (suburban, high) we can calculate it as: \\(P(urban, high \\cup suburban, high) = P(urban, high) + P(suburban, high) = 0.24 + 0.22 = 0.46\\) This seems a bit unnatural at first because it doesn’t, on the surface, appear to be the same as flipping a coin. But the fact that there are 6 numbers (5 of which are independent, the 6th is derivable from the axiom #2) instead of 1 doesn’t change anything. The main differences are: The process involves the development of geographic areas (instead of the physical process of flipping a coin). There are two (joint) events being measured (instead of just the one, heads or tails). As it turns out, calculating these probabilities from data is one of the simplest forms of modeling. We will discuss this later. On a final note, there is no limit (except perhaps processing power, space, or credulity) to the number of variables you can have in a joint probability distribution. \\(P(A, B)\\) is one. \\(P(A, B, C)\\) is one. And \\(P(A, B, C, D, E, F, G)\\) is one. Of course, as you add more variables, the size of the corresponding probability table increases exponentially. 4.4.2 Conditional Probability \\(P(C, I)\\) is a joint probability distribution over all the possible combinations of Community and Income. But what if we already know the Income? This is denoted by \\(P(C | I)\\) and is called a conditional probability distribution because we “condition” our uncertainty on the information we have. When we look at a specific event, a joint probability might be something like \\(P(urban, low)\\). This is the probability of the next geographic area being both an urban community and low income if we don’t know either value. However, the conditional probability \\(P(urban | low)\\) is the probability of the next geographic areas being both an urban community and low income if we already know that it’s low income. These two probabilities will not necessarily be the same (a point to which we will return later). Conditional probability effectively focuses our attention on a subset of the joint probability space where some of our uncertainty is resolved by knowing some of what happened. Remember that joint probability is the probability of any combination of events, assuming we know nothing. But if we know that \\(income = low\\) then we do not need to look at \\(income = high\\) anymore because it’s not possible. We know that that event is not going to happen and thus we can ignore it. This is true whether there are 2 possible events or 100 and whether we know the values for 1 or 99. Whenever we find out the value of a variable–that some event has happened–we can ignore all other values for that variable as no longer being possible. When we do this, however, we’re left with improper probability distributions. By the 2nd Axiom of Probability, all probability distributions must sum to 1 therefore we need to normalize the remaining probabilities (of event combinations that can happen). We do this by dividing through by the sum of the remaining possible events. More formally, the definition of conditional probability is: \\(P(A|B) = \\frac{P(A, B)}{P(B)}\\) and if you play with the math a bit, you will see that this is the same thing. Don’t be confused by the seeming division of a probability distribution by another probability distribution. This is just a shorthand for element-wise division. Therefore, using the joint probability table from above, we can calculate: \\(P(urban|low) = P(urban, low)/P(low) = 0.29/0.52 = 0.56\\) Our conditional probability table, \\(P(Community|Income)\\), describes the uncertainty under any outcome for Income: Conditional Probability, \\(P(Community|Income)\\) area low high rural 0.08 0.05 suburban 0.36 0.46 urban 0.56 0.50 In this case, \\(P(Community|Income)\\) has 2 probability distributions (one for each possible value of Income). In this case, each column sums to 1. We can do this for Community as well: Conditional Probability, \\(P(Income|Community)\\) area low high rural 0.65 0.35 suburban 0.46 0.54 urban 0.54 0.46 But this means that the table above actually has three separate probability distributions. One for each possible outcome of Community and each distribution itself (row in this case) adheres to Axiom #2 (they sum to 1). In general, with conditional probabilities, knowing that some event occurs often changes our information about the certainty or uncertainty of another event. However, in general, we cannot say anything about whether or not: \\[P(A) &lt;=&gt; P(A | B)\\] Conditional probabilities are perfectly general. Just as we can have a joint probability distribution over many different event spaces such as P(A, B, C, D, E, F) we can have a conditional probability distribution where the result or value for some (at least one) of the sets is known. For example, we might know the values of \\(C\\) and \\(D\\): \\(P(A, B | C, D) = \\frac{P(A, B, C, D)}{P(C, D)}\\) 4.4.3 Marginal or Prior Probability When dealing with a joint probability distribution over many sets, there may sometimes be sets that we don’t care about, that is, we’re only interested in the probabilities of some subset of joint event space. The simplest example is having a joint probability distribution \\(P(A, B)\\) and only being interested in the probability distribution \\(P(A)\\) or \\(P(B)\\). In this case, we can marginalize out the events we are not interested in. “Marginalization” comes from the actual practice of calculating marginal values/probabilities in the actual margins of books and ledgers. For example, if we have our joint probability distribution \\(P(C, I)\\) and we only care about community, \\(C\\), we can marginalize out income: \\(P(urban, low \\cup urban, high) = P(urban, low) + P(urban, high) = 0.29 + 0.24 = 0.53\\) \\(P(suburban, low \\cup suburban, high) = P(suburban, low) + P(suburban, high) = 0.19 + 0.22 = 0.41\\) \\(P(rural, low \\cup rural, high) = P(rural, low) + P(rural, high) = 0.04 + 0.02 = 0.06\\) which leads to the following: Marginal Probability, \\(P(Community)\\) area any rural 0.06 suburban 0.41 urban 0.53 This follows from the axioms of probability. We can also do this for income: Marginal Probability, \\(P(Income)\\) area low high any 0.52 0.48 which strangely leads us back to coin flipping. According to Jaynes, you can think of P(heads) as a marginalization over everything we don’t know or don’t care about in a larger joint probability distribution such as P(heads, gravity, Earth, Jim, left handed, …). 4.5 Rules of Probability There are some handy rules that follow from the axioms of probability. In a probability course, you might be required to prove them. We will present them without proof. 4.5.1 Monotonicity \\(A \\subseteq B \\Rightarrow P(A) \\leq P(B)\\) This says that if A is a subset of B then the probablity of A must not exceed the probability of B. This is really an abuse of notation. What we really mean is: \\(A \\subseteq B \\Rightarrow \\sum_i P(a_i) \\leq \\sum_j P(b_j)\\) 4.5.2 Negation \\(P(\\neg{a}) = 1 - P(a)\\) This follows from axiom #2. If we write out the summation and isolate the single event \\(a\\) and then re-arrange terms, we get the above rule. 4.5.3 Total Probability This is also called the Law of Total Probability. It has the form: \\(P(A) = \\sum_i P(A|B=b_i) P(B=b_i)\\) Remember that P(A) is a set of probabilities (one for each element in A). This rules makes a bit more sense if you break it out: \\(P(A=a_1) = P(A=a_1|B=b_1) P(B=b_1) + P(A=a_1|B=b_2) P(B=b_2) + \\ldots + P(A=a_1|B=b_n) P(B=b_n)\\) We can derive the Law by looking at the definition of conditional probability for a single event: \\(P(A=a_1|B=b_1) = \\frac{P(A=a_1, B=b_1)}{P(B=b_1)}\\) and re-arranging terms: \\(P(A=a_1, B=b_1) = P(A=a_1|B=b_1)P(B=b_1)\\) From our discussion about marginalization and Axiom #2, we know that: \\(P(A=a_1) = P(A=a_1, B=b_1) + P(A=a_1, B=b_2) + \\ldots + P(A=a_1, B=b_n)\\) By substitution, we have: \\(P(A=a_1) = P(A=a_1|B=b_1) P(B=b_1) + P(A=a_1|B=b_2) P(B=b_2) + \\ldots + P(A=a_1|B=b_n) P(B=b_n)\\) and for the entire set A, we have: \\(P(A) = SUM_i P(A|B=b_i) P(B=b_i)\\) Total Probability is very useful (believe it or not) because there are many situations where you don’t know P(A) but you know P(A|B) and P(B). We will see this later. 4.5.4 Chain Rule Again, starting with the definition of conditional probability we have: \\(P(A|B) = \\frac{P(A, B)}{P(B)}\\) and by re-arranging we have: \\(P(A, B) = P(A|B)P(B)\\) We can expand this to more sets: \\(P(A,B,C) = P(A|B,C) P(B|C) P(C)\\) \\(P(A,B,C,D) = P(A|B,C,D) P(B|C,D) P(C|D) P(D)\\) We can expand in any order: \\(P(A,B,C,D) = P(D|A,B,C) P(B|A,C) P(A|C) P(C)\\) But this is really just a clever manipulation of the definition of conditional probability: \\(P(A,B,C,D) = P(D|A,B,C) P(B|A,C) P(A|C) P(C)\\) \\(P(A,B,C,D) = \\frac{P(D,A,B,C)}{P(B,A,C)} \\frac{P(B,A,C)}{P(A, C)}\\frac{P(A, C)}{P(C)} P(C)\\) Still, it can be a handy thing to know and it presents the foundation for Bayesian Networks. 4.5.5 Bayes Rule Bayes Rule is a similar manipulation of conditional probability. We start with the definition of conditional probability: \\(P(A|B) = \\frac{P(A,B)}{P(B)}\\) and re-arrange: \\(P(A,B) = P(A|B)P(B)\\) We can start with the “opposite” conditional probability: \\(P(B|A) = \\frac{P(A,B)}{P(A)}\\) and re-arrange: \\(P(A,B) = P(B|A)P(A)\\) which means I can set these two equal to each other: \\(P(B|A)P(A) = P(A|B)P(B)\\) and re-arrange: \\(P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\\) which does not look particularly interesting until I start giving my sets interesting names: B=Hypothesis and A=Data: \\(P(H|D) = \\frac{P(D|H)P(H)}{P(D)}\\) which says…the probability of the hypothesis (or model or parameter) given the data is equal to the probability of the data given the hypothesis times the probability of the hypothesis. This is the normalized by the probability of the data. These probabilities all have names: P(H|D) = posterior probability P(D|H) = likelihood P(H) = prior probability P(D) = normalizer Note that we almost never know P(D) and we will often use total probability to calculate it: \\(P(D) = \\sum_h P(D|H=h)P(H=h)\\) Remember what our notation means. By solving for \\(P(H|D)\\) we are solving for all the hypotheses and all the data outcomes at once. This means the posterior distribution described by \\(P(H|D)\\) is not a single probability distribution but many. For a single hypothesis, we would have something like: \\(P(h1|D) = \\frac{P(D|h1)P(h1)}{P(D)}\\) Note that the denominator does entail all hypotheses. We often end up having to use the rule of Total Probability here: \\(P(h1|D) = \\frac{P(D|h1)P(h1)}{P(D|h1)P(h1) + P(D|h2)P(h2) + \\dots + P(D|h_n)P(h_n)}\\) Bayes Rule is particularly important to data science because it says exactly how we should change our degree of certainty given new evidence. It also plays a foundational role in several machine learning techniques (Naive Bayes Classifier, Bayesian Belief Networks). It is also the main formula for Bayesian inference. We will spend an entire later chapter on Bayes Rule because of its central role in inference. 4.6 Independence and Conditional Independence From before, we manipulated the definition of conditional probability as follows: \\(P(A|B) = \\frac{P(A,B)}{P(B)}\\) and re-arrange: \\(P(A,B) = P(A|B)P(B)\\) Remember our interpretation of conditional probability: knowing what event in B happened gives us additional information that influences our expectations about which event in A will happen. If it doesn’t, then: \\(P(A, B) = P(A)P(B)\\) in which case A and B are said to be independent. This is known as the Multiplication Rule of Probability. In the previous section on Probability Rules, we introduced the Chain Rule. The Chain Rule basically gives us permission to factor a joint probability distribution however we please. For example, we might have: \\(P(A, B, C) = P(A|B, C)P(B|C)P(C)\\) we can also do the same with a conditional joint probability: \\(P(A,B| C) = P(A|B, C)P(B|C)\\) This says we can factor our uncertainty in the joint events of A and B given some known event in C into two parts: the uncertainty over an event in A given events in B and C are known times the uncertainty in an event in B given the event in C is known. For a concrete example, consider a joint probability of Religion, Party and Wealth. That’s \\(P(A, B, C)\\) or \\(P(Party, Wealth, Religion)\\). Now suppose we condition on \\(Religion\\) so we have \\(P(Party, Wealth | Religion)\\). We’re now describing our uncertainty over someone’s Party and Wealth given we know their Religion. What the above says is that we can factor that into two parts. The first part is the probability of Party given we know their Wealth and Religion and the second part is the probability of Wealth given we know their Religion. Returning to the example, however, if the following is true: \\(P(A,B|C) = P(A|C)P(B|C)\\) then we can say that A and B are conditionally independent given C. Note that the above can be true and that the following is also true: \\(P(A, B) \\neq P(A)P(B)\\) The one does not imply the other. There are many applications in statistics and machine learning that either require or assume independence or conditional independence. 4.7 Probabilistic Fallacies As it turns out, people in general are not particularly good with probabilities and make quite a few mistakes. These mistakes are fallacies in probabilistic reasoning just as there are reasoning and argument errors in general (for example, ad hominem arguments). 4.7.1 Conjunction Fallacy Monotonicity plays a very important role in judging the probabilities of events. Perhaps the most famous example arises in the form of the Conjunction Fallacy illustrated as follows: Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice and participated in anti-nuclear demonstrations. Which is more probable? Linda is a bank teller. Linda is a bank teller and active in the feminist movement. The majority of people, when asked, picked #2 but this cannot be. If B is the set of all tellers then the set of feminist bank tellers is likely smaller, a subset of A of B. It cannot be larger. Being an element of A cannot be more probable than being an element of A and B. This basically says that \\(P(A=a) &lt;= P(A=a, B=b)\\). Note that this is not the same thing as what we said before with conditional probability because we are talking about joint probability here. The main problem here actually seems to be confusing joint and conditional probabilities. Still, generally, the more specific you are, the less probable it is. We will see later that this is important for designing experiments and analyzing data. 4.7.2 Gambler’s Fallacy and Hot-Streak Fallacy A common fallacy is that there is some sort of overarching or underlying power or force that keeps probabilities in balance. The most common way that this fallacy shows itself is as the Gambler’s Fallacy. In the Gambler’s Fallacy, the person incorrectly believes that because a rare event has not happened, it’s time must come or it must happen in order to balance the probabilities in some way. For example, if “red 13” has not come up in Roulette for a while, then this “must” happen any time now. The Hot-Streak Fallacy is similar in that it assumes that some extraordinary event must continue to happen. However, some research has shown that in certain cases, this is not necessarily true “Hot Hand” is not an illusion. This is a very easy mistake to make so Data Scientists and consumers of Data Science must be especially wary of this particular fallacy. 4.7.3 Inverse Probability Fallacy The Inverse Probability Fallacy relates to conditional probabilities, specifically by believing that the following are the same: \\(P(A|B) = P(B|A)\\) A concrete example of this fallacy would involve making the following claim: \\(P(rain|prediction) = P(prediction|rain)\\) Here we are saying that the probability of rain given that rain was predicted is equal to the probability of a prediction for rain given that it is raining. These are two entirely different things. Remember that probability is just counting and normalization. In the first case, we count all the times where it rained given a prediction of rain. In the second case, we count all the times rain was predicted given that it rained. You can think of a table like so: actual prediction yes no yes yes no yes yes no In the first case, we add up all the times where actual is yes and prediction is yes but divide by the times that prediction is yes (regardless of actual). In the second case, we add up all the times where actual is yes and prediction is yes but divide by the times that actual is yes (regardless of prediction). These will not necessarily lead to the same number. 4.7.4 Base Rate Fallacy Suppose we’re asked what religion we think Garth is and, knowing that he’s from middle America, we can guess that he’s notionally a Christian. Suppose we further learn that Garth is a goth and wears dark clothing with various mystical symbols on it, our estimation of Garth’s religion would probably swing in the direction of being a Satanist. However, the base rate (prior) of being a Satanist is really quite low. There are 2,000,000,000 Christians in the world and only 100,000s of Satanists. While the probability of Garth being a Christian might go down, knowing that Garth is a goth shouldn’t really flip our sense of the probability from most likely a Christian to most likely a Satanist. Doing this is called the Base Rate Fallacy. This is also related to Bayes Rule which tells us exactly how much we should change our prior when reviewing evidence. The difference between the posterior and prior is the incremental evidence whereas the posterior alone is the total evidence. The Base Rate fallacy can also be attributed to confusing incremental evidence and total evidence when the base rate is low. Upon learning that Garth dresses in black, wears eyeliner and black fingernail polish, we adjust our beliefs about his religion. But to conclude that he’s a Satanist, ignoring the base rate, is to improperly change our beliefs based on the evidence alone. 4.7.5 Prosecutor’s Fallacy This fallacy refers to a Prosecutor (and sometimes Defense Attorneys) arguing the wrong thing. This can result in a mistrial. As it turns out this rarely happens among the attorneys but can happen to expert testimony. This is really a family of fallacies the first of which relates to Bayes Rule and is related to the Inverse Probability Fallacy: \\(P(innocence|evidence) = \\frac{P(evidence|innocence)P(innocence)}{P(evidence)}\\) The Fallacy is committed when the prosecutor assumes that just because the damning evidence is small \\(P(evidence|innocence)\\) (“if he were innocent, the evidence would be really unlikely”) that \\(P(innocence|evidence)\\) must be equally as small. This happens a lot with forensic evidence, especially DNA evidence. But it simply isn’t true that if \\(P(evidence|innocence) = 1:1,000,000\\) that \\(P(innocence|evidence) = 1:1,000,000\\). Another version of the Fallacy confuses the prior and conditional probabilities, that is, it assumes that \\(P(A)=P(A|B)\\) and is thus related to the Base Rate Fallacy. 4.8 Applications We can solidify our understanding of probability, conditional probability and Bayes Rule by going over some problems, some of which are quite famous. We’ll start first with some general probability problems and then move in the second part to problems using Bayes Rule. Most of these problems come from Allen Downey’s excellent book Think Bayes. Some of these problems are just calculations, either by hand or by computer. Others involve answering questions with simulations in order to calculate probabilities. You may be called upon to do something similar as a data scientist. For example, you have just fielded an advertising campaign in 20 major cities. Testing showed that advertising campaign was 10 percent better than the previous one. However, the last three weeks of returns in New York have been below average. What is the probability of this happening given that the campaign really is 10 percent better? 4.8.1 Applications in General Probability These problems are general probability problems (although the Monty Hall problem can be solved using Bayes Rule). 4.8.1.1 A Girl Named Florida Consider the following problems: we have a family with two children. What is the probability that they are both girls? To answer this question, we have have to make some assumptions about the probability of a child being either a boy or girl (which we will take to mean either XX or XY chromosomes). The generally accepted probabilities are P(boy) = 0.5 and P(girl) = 0.5 (ignoring other chromosomal possibilities). Recall our definition of independence. Two sets of events, \\(A\\) and \\(B\\), are independent if the following holds true: \\[P(A, B) = P(A)P(B)\\] but that’s how we determine that A and B are independent. If we assume that the events are independent, then we can turn it around to calculate the probability of the joint event: \\[P(A)P(B) = P(A, B)\\] Here \\(A\\) is the “sex of the first child” and \\(B\\) is the “sex of the second child”. This means we can take P(A=girl) as 0.5 and P(B=girl) as 0.5–shortened to P(girl) x P(girl)– which equals 0.5 x 0.5 = 0.25. Note that if we wanted to calculate the probability of them being different sexes, then we’d have to calculate the probability of having a boy then a girl (0.5 x 0.5 = 0.25) and the probability of having a girl then a boy (0.5 x 0.5 = 0.25) and combine them based on the Additive Law of Probability: 0.25 + 0.25 = 0.50. But remember how I said probability is just counting? There are 4 possibilities: Boy, Girl. Girl, Boy. Girl, Girl. Boy, Boy. There is only one way in which both children are girls so the probability of two girls is 1/4 = 0.25. There are two ways in which the children are mixed sexes so the probability of that joint event is 2/4 = 1/2 = 0.5. What is the probability that they are both girls given that at least one is a girl? Now there are only 3 possibilities–the ones that include a Girl as either the first or the second birth: Boy, Girl. Girl, Boy. Girl, Girl. Using the counting method, we can see that there is only one way to get the result we’re interested in and three possible outcomes so the probability is 1/3. Using the mathy way, we know from above that probability of the individual outcomes are each 1/4 or 0.25: Boy, Girl = 1/4 Girl, Boy. = 1/4 Girl, Girl. = 1/4 However, since we have ruled out the {Boy, Boy} possibility by assumption, we have to renormalize the probabilities. Normalization just means “make all the probabilities add up to 1 again” and you do this by adding the probabilities together (which is 3/4) and dividing each original probability by this normalizer: Boy, Girl = 1/4 // 3/4 = 1/3 Girl, Boy. = 1/4 // 3/4 = 1/3 Girl, Girl. = 1/4 // 3/4 = 1/3 And we get 1/3 as before. The reason we show both ways to get the answer is because, as you might expect, there are cases–most cases–where the counting approach isn’t tractable. What is the probability that they are both girls given that the oldest (first) is a girl? We do the same thing again except that any outcome that has a Boy as the oldest is removed: Girl, Boy. = 1/4 Girl, Girl. = 1/4 And again, we need to have our probabilities add up to 1 so we normalize them: Girl, Boy. = 1/4 // 2/4 = 1/2 Girl, Girl. = 1/4 // 2/4 = 1/2 If you think about it a second, this makes perfect sense…the events are independent so knowing that the first is a girl doesn’t give us any information about the second child’s sex. That was a fairly typical probability problem. There is a crazy variant that asks: What is the probability that they are both girls given that one of them is a girl named Florida? Think about it. Does the name change anything? We’re now going to switch to problems where simulation is often a useful tool. If you ever have a probability problem that you can’t quite formulate right or if someone doesn’t believe your answer, think: can I simulate this? 4.8.1.2 Birthday Problem The Birthday Problem is as follows: what is the probability that two people in a given group of size \\(N\\), have the same birthday (month and day)? Guess. What do you think the probability is? 10%, 20%, 30%…100%? Think about how you might answer this mathematically. Think about how you might solve this easily as a simulation. What assumptions do you need to make? We’re going to simulate the problem by writing a few functions. The first function takes \\(k\\) persons as an argument and assigns them randomly to one of the 365 days of the year (we ignore leap years). As we do so, we count how many people have that birthday. First some imports… from random import randint, uniform from collections import defaultdict We used defaultdict because missing keys are automatically assigned a value of 0 instead of it causing a KeyError. def tally_k_birthdays( k): counts = defaultdict( int) for i in range( 0, k): birthday = randint( 1, 365) counts[ birthday] += 1 return counts Let’s see what we get for 10 people: tally_k_birthdays( 10) Now all we need to do is take this dictionary of values and see if any of the days (we only need one) has a count greater than one which would mean that two (or more) people have the same birthday: def identify_same_birthdays( counts_of_birthdays): for day in counts_of_birthdays.keys(): if counts_of_birthdays[ day] &gt; 1: return True return False In general, in order to get a good result from a simulation, it must be run multiple times and the results averaged. We write a function to do just that. The arguments are \\(N\\) people and \\(times\\) simulations. def sample_group( N, times): match = 0.0 for i in range( times): birthday_count = tally_k_birthdays( N) if identify_same_birthdays( birthday_count): match += 1.0 return match / times We can now run the function and see approximately what the probability is for two people to have the same birthday in a class with \\(N=26\\) students: sample_group( 26, 10000) It’s much more probable than people usually think. This is a good example of a simple simulation for a system process. Again, in theory, everything is fairly deterministic. Parents decided to have children, the children were born on certain days, the children grew up and where in a particular class (one such situation) or they got older and went to university (another situation) or took up an interest in art and when to an art gallery (another such situation) and in all cases the simulation works. It doesn’t work if an assumption if violated. If the situation is a Meetup for People born in March, we would need an entirely different situation. Can you reprogram the simulation to see how many people it takes to have a 50% probability of someone with the same birthday, if everyone is born in the same month? 4.8.1.3 Monty Hall Problem Monty Hall was the host for Let’s Make a Deal before Wayne Brady. One of the “bits” on the show involved picking a curtain in hopes of winning a great prize like a car and this probability problem is based on it. It’s actually a very famous problem. There are three curtains: 1, 2, and 3. Behind one of those curtains is a car. On the show, the other curtains often had gag gifts behind them like a goat but we assume they’re empty. The contestant picks the curtain they believe hides the car. After picking, Monty reveals what is behind one of the other curtains. One important assumption is that if the contestant has picked the car, Monty reveals one of the other two curtains at random. The contestant is then given the option to either stick with the curtain they picked or switch to the remaining curtain. The question is this: should the contestant switch? What do you think? There are a number of ways to answer this question but we’re going to use simulation because that’s often the most definitive. In fact, Paul Erdos, the famous mathematician, would not believe the correct answer until it was simulated. First, we have a function that simulates one Monty Hall “Problem”. It basically says: set up the problem place the car at random. generate a random contestant pick. figure out which curtain to reveal. figure out which curtain is closed. if do_switch is True, make the pick equal to the closed curtain. Otherwise, keep it the same. return if the picked curtain equals the car’s curtain (True or False). def evaluate_a_monty_hall_scenario( do_switch=False): options = {1, 2, 3} car = randint( 1, 3) pick = randint( 1, 3) opened = list( options.difference( {car}).difference( {pick}))[0] closed = list( options.difference( {pick}).difference( {opened}))[0] if do_switch: pick = closed return car == pick Let’s run it 10 times: for i in range( 0, 10): print( evaluate_a_monty_hall_scenario(True)) We’re now going to run the Monty Hall problem function 10,000 times and evaluate what happens first, if you don’t switch and second, if you switch: def evaluate_monty_hall_problem( switch=False): trials = 10000 count = 0 for i in range( 0, trials): result = evaluate_a_monty_hall_scenario( switch) if result: count += 1 return float( count) / trials evaluate_monty_hall_problem() evaluate_monty_hall_problem(True) And there you have it, if you switch, you win the car 66% of the time. 4.8.2 Applications of Bayes Rule Speaking of switching, one of the main types of problems we’ll be solving are problems involving Bayes Rule. In fact, Bayesian Inference depends entirely on understanding Bayes Rule and evaluating it for a large number of possibilities. We’ll start out with smaller problems. For whatever reason, Bayes Rule examples are either weather or medical tests. We’ll start with the weather: 4.8.2.1 Rain or Shine Sam is getting married tomorrow in an outdoor ceremony in the desert. In recent years, it has only rained 5 days per year. Unfortunately, the meteorologist has predicted rain for tomorrow. Should Sam rent a tent for the ceremony? We can solve this problem using Bayes Rule which remember is: \\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\] But instead what we want is: \\[P(W|F) = \\frac{P(F|W)P(W)}{P(F)}\\] where \\(W\\) is weather (rain or shine) and \\(F\\) is forecast (rain or shine). Remember that \\(P(W)\\) in the numerator is our prior probability. What is our prior probability? Well, it only rains 5 days a year on average: rain shine 5/365 = 0.0137 360/365 = 0.9863 I think this is what Sam had in mind when he planned his wedding. But now he needs to take new evidence into account: a forecast of rain. The likelihood \\(P(F|W)\\) is essentially the probability of the meteorologist being correct: given that it rained, what is the probability that it was forecast? Sam looks this up on the Internet. F rain shine rain 0.8 0.2 shine 0.2 0.8 What does this mean? Given that it rained, there is an 80% chance there was a forecast of rain: \\(F(F=rain|W=rain) = 0.8\\) Because it will be confusing, we do not take shortcuts here. We will use the longhand notation, F=rain and W=rain, to distinguish the two events. Up above, we had Bayes Rule defined over entire random variables. What does it look like for the specific outcome we’re interested in? \\[P(W=rain|F=rain) = \\frac{P(F=rain|W=rain)P(W=rain)}{P(F=rain)}\\] We have everything we need except the denominator. We can use total probability for it, though: \\(P(F=rain) = P(F=rain|W=rain)P(W=rain) + P(F=rain|W=shine)P(W=shine)\\) \\(0.8 \\times 0.0137 + 0.2 \\times 0.9863 = 0.208\\) and now we have: \\(P(W=rain|F=rain) = \\frac{0.8 \\times 0.0137}{0.208} = 0.053\\) So really, Sam should just go ahead with the wedding (at least from a weather perspective). 4.8.2.2 Breast Cancer The logic underlying this problem is why certain routine screenings for breast cancer were discontinued. The numbers, however, are made up. 1% of women at age 40 who participate in routine screening have breast cancer. 80% of women with breast cancer will get positive mammographies. 9.6% of women without breast cancer will also get positive mammographies. A woman in the age group had a positive mammography. What is the probability of her having breast cancer? We have two variables, each with two outcomes: \\(M\\) is {pos, neg}, and \\(C\\) is {yes, no}. As before, we need to set up Bayes Rule and determine either what information we have and what information we need to calculate. \\[P(yes|pos) = \\frac{P(pos|yes)P(yes)}{P(pos)}\\] We have the prior, \\(P(yes)\\) which is simply 0.01. We have the likelihood we need which is established in the second sentence: \\(P(pos|yes)\\) = 0.8 (which means that \\(P(neg|yes)\\) = 0.2. We don’t have \\(P(pos)\\). We will need to use total probability again. \\(P(pos) = P(pos|yes)P(yes) + P(pos|no)P(no)\\) We have \\(P(pos|no)\\) from the 3rd sentence: 0.096. Note that this clearly shows where total probability comes from. If we want to calculate the probability of a positive test result, we need to take into account all the possible sources of positive test results. These come from those with cancer who get a positive test result (the first term) and those without cancer who get a positive test result (the second term). The probability of not having cancer is just 1 - P(yes). \\(P(pos) = 0.8 \\times 0.01 + 0.096 \\times 0.99 = 0.103\\) and now we can just plug in the numbers. \\(P(yes|pos) = \\frac{0.8 * 0.01}{0.103} = 0.078\\) This result makes an important assumption, though, the only information about this woman’s status is that this was a routine screening. Why might this not be the case? OK, we’re computer scientists…enough math. We can let computers do the math. 4.8.2.3 Elvis Apparently Elvis was one of a set of twins. He had a twin brother who died at birth. We want to know the probability that Elvis had an identical twin. This isn’t really enough information to answer anything so… Wikipedia to the rescue…“Twins are estimated to be approximately 1.9% of the world population, with monozygotic twins making up 0.2% of the total, 8% of all twins”. You should solve this by hand right now, writing out the problem. It might surprise you how difficult it is to get started. Consider the following…what is the event we want to know about and what is the evidence? So the evidence is that the child was male and the event we’re trying to determine the probability of is that Elvis and the child were identical twins: \\[P(I|M) = \\frac{P(M|I)P(I)}{P(M)}\\] I’m going to start out with a helper function that normalizes a probability distribution the way I have decided to represent it (as a map): def normalize( dist): normalizer = sum( dist.values()) for k in dist.keys(): dist[ k] = dist[ k] / normalizer return dist ## don&#39;t need to do this. I’m describing the events as Identical twin or Fraternal twin. The probabilities come from the Wikipedia article. In Python, it is very convenient to represent a discrete Probability distribution with a Dict where the keys are outcomes {“I”, “F”} and the values are the probabilities of those outcomes. elvis_prior = {&quot;I&quot;: 0.08, &quot;F&quot;: 0.92} Here we use a Dict to express a likelihood which ends up as a nested Dict. Remember that \\(P(A|B)\\) is a Probability distribution for each value of “B”. In this case, the outer key is the “given” so that we can say “given I” and look up the appropriate probability distribution. The inner Dict represents the probability distribution over the events of “A”, in this case the sex of the baby, Male or Female. elvis_likelihoods = { &quot;I&quot;: { &quot;M&quot;: 1.00, &quot;F&quot;: 0.00}, &quot;F&quot;: { &quot;M&quot;: 0.50, &quot;F&quot;: 0.50} } Below is a function that will calculate the posterior probability for the entire probability distribution (over all events). As we’ve mentioned before, in Bayes Rule: \\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\) we are calculating an entire posterior probability distribution…a probability for each value of A given each value of B. Additionally, it is unlikely that we know the value of the normalizer \\(P(B)\\) directly. However, we can calculate \\(P(B)\\) using the Rule of Total Probability: $P(B) = P(B|A=a_1)P(A=a_1) + P(B|A=a_2)P(A=a_2) + … + \\(P(B|A=a_n)P(a_n)\\) but it turns out that if we are interested in the probability of every hypothesis in A, we are going to calculate all of these values anyway. We don’t need to go through any extra effort. First we note that if we are only concerned about order we do not need to normalize so we have: \\(P(A=a_1|B) \\propto P(B|A=a_1)P(A=a_1)\\) \\(P(A=a_2|B) \\propto P(B|A=a_2)P(A=a_2)\\) \\(P(A=a_n|B) \\propto P(B|A=a_n)P(A=a_n)\\) where \\(\\propto\\) means “proportional to”. We can calculate all of these without calculating the normalizer, \\(P(B)\\). But having calculated all those terms, we have calculated the terms we need to compute the normalizer and calculate the actual probabilities: \\(P(A=a_1|B) = \\frac{P(B|A=a_1)P(A=a_1)}{P(B)}\\) \\(P(A=a_2|B) = \\frac{P(B|A=a_2)P(A=a_2)}{P(B)}\\) \\(P(A=a_n|B) = \\frac{P(B|A=a_n)P(A=a_n)}{P(B)}\\) This is what the following function does, although for all values of A and B. def query( prior, likelihoods, evidence): posterior = {} for k in prior.keys(): posterior[ k] = likelihoods[ k][ evidence] * prior[ k] normalize( posterior) return posterior Now we can print out the prior probability and the posterior probability: print( &quot;prior=&quot;, elvis_prior) print( &quot;posterior=&quot;, query( elvis_prior, elvis_likelihoods, &quot;M&quot;)) The evidence (that the other child was a boy), increases the probability that they were identical twins (if the other child had been female, it would have been impossible). What other piece of evidence is implicit in this calculation? 4.8.2.4 M &amp; M’s Here is a bit more challenging problem. A friend shows me two bags of M&amp;M’s and tells me that one is from 1994 and the other is from 1996. He won’t tell me which is which but gives me an M&amp;M from each bag. Which bag is which? So the first step is map out the events we’re trying to predict and the evidence. I’ll use the same basic approach as before, representing probability distributions as Dicts. The key information, however, is that the blue M&amp;M was introduced in 1995. Before that the color mixes in the bags where: color 1994 1996 brown 30% 13% yellow 20% 14% red 20% 13% green 10% 20% orange 10% 16% tan 10% 0% blue 0% 24% (I’m not sure where this data came from!) You should try to solve this for yourself before looking at my solution. Here is the prior distribution for the 1994 bag: mix94 = dict(brown=0.3, yellow=0.2, red=0.2, green=0.1, orange=0.1, tan=0.1) mix94 and the prior distribution for the 1996 bag: mix96 = dict(blue=0.24, green=0.2, orange=0.16, yellow=0.14, red=0.13, brown=0.13) mix96 Now, my two possible events are: either the first bag is the 1994 bag (A) or the first bag is the 1996 bag (B): A = dict(bag1=mix94, bag2=mix96) B = dict(bag1=mix96, bag2=mix94) which gives me my likelihoods: m_m_likelihoods = {&quot;A&quot;: A, &quot;B&quot;: B} m_m_likelihoods This is a more complex likelihood than we’re used to seeing. Given that event A happened (1994 bag), then the probability of picking a yellow M&amp;M from that bag is 20%. Given that event B happened (1996 bag), then the probability of picking a yellow M&amp;M out of that bag is 14%. Our prior is 50/50 for each of the events A and B because there are two bags. m_m_priors = {&quot;A&quot;: 0.5, &quot;B&quot;: 0.5} Our evidence is that I took a yellow M&amp;M out of Bag 1 and a green M&amp;M out of Bag 2: m_m_evidences = [(&#39;bag1&#39;, &#39;yellow&#39;), (&#39;bag2&#39;, &#39;green&#39;)] And now some code to massage it all together: from copy import deepcopy def calculate_m_m_posteriors( priors, likelihoods, evidences): posteriors = {} current_priors = deepcopy( priors) for evidence in evidences: bag, mnm = evidence for hypothesis in priors.keys(): posteriors[ hypothesis] = likelihoods[ hypothesis][ bag][ mnm] \\ * current_priors[ hypothesis] normalize( posteriors) current_priors = posteriors print( &quot;evidence=&quot;, evidence, &quot;posterior=&quot;, posteriors) return posteriors print( &quot;prior&quot;, m_m_priors) calculate_m_m_posteriors( m_m_priors, m_m_likelihoods, m_m_evidences) Based on the evidence, the more likely event is “A”…that the first bag is the 1994 bag of M&amp;Ms and the second bag is the 1996 bag of M&amp;Ms. One special thing to note about Bayes Rule is that it doesn’t matter if you take the evidence altogether or piece by piece (no pun intended). You will always get the same result. It’s slightly easier in this case to cycle through the evidence and use the posterior distribution that results as the prior distribution for the next calculation. This is the beauty of Bayes Rule (and makes it slightly easier to program). It is always worth noting that in all cases we used probability to deal with systems–processes–exhibiting uncertainty whether it was a breast cancer testing process, the weather, Elvis’s deceased twin or M&amp;M’s. Can you solve the Monty Hall problem using Bayes Rule? Can you identify–even in the most general terms–the processes underlying each of these problems? 4.9 Probability and Simulation Now it may turn out that you cannot easily describe your problem analytically or enumerate all the possibilities in which case you can turn to simulation to do you event generation and counting, to get the probabilities you’re interested in. Here’s an example that actually happened to me. 4.9.1 Cities Some data scientists are involved in personalization at online retailers. They apply personalization models to different views of the products whether they are sales emails or on the website; anywhere there are default lists of products. Of course, not being content to rest on their laurels, these data scientists are constantly developing new personalization models. In order to test whether a new model is better than an old model, they engage in A/B testing. We’ll talk about A/B testing later in the book but for now, A/B testing just means dividing your, ahem, test subjects into two groups completely at random. Group A will get the control or existing personalization model. Group B will get the treatment or new personalization model. That sets up the context for the following problem in computational probability. Suppose you are one of those data scientists and have a personalization model that is being tested in 20 cities nationwide. Each city has 100,000 customers on the mailing list and you send an email every day. The test has been running a few days and it looks like the new model (called the “treatment”) is about 10% better than the current practice (“control”). Let us assume this is true (we’ll see later how to test this). After another week of testing, the director of marketing comes to you and is concerned that the new model has done very poorly in one of the cities 3 days in a row and opines that it must not really be 10% better. Is the director of marketing right? Does this mean the new model is worse than the current one? Maybe, maybe not. Deciding which model is better is a problem in inference. This isn’t really what the director of marketing is asserting, though. They’re asserting that the new model can’t be better if there is a losing streak of 3 days in one city. Now this is a question we can answer now, using computational probability. What we want to know–what the stakeholder needs to know–is, if the new model is definitely 10% better, what is the probability of seeing a string of 3 worse outcomes in a particular city? If the probability is high, then we needn’t worry. If the probability is low, we might be concerned about test. 4.9.2 What do we know? 20 cities Each city has 100,000 subscribers with the lists split in half for control and treatment. the purchase rate for control is 0.0001 (0.01%) the purchase rate for treatment is 0.00011 (0.011%) the lift is 0.011/0.010 = 1.1 - 1.0 = 10% These probably seem small but daily purchase rates are often small. Let’s start out by simulating a single day’s worth of purchases in a single city. We know what the ideal purchase rate is, but it’s not going to pan out to be the same exact thing everyday. We need to simulate those purchases and calculate the actual purchase rate: from random import random, seed seed(128934662) def actual_purchase_rate( population, purchase_rate): purchases = [1.0 if random() &lt; purchase_rate else 0.0 for i in range( population)] return sum( purchases)/population Let’s see how it does: for _ in range( 5): print( actual_purchase_rate( 50000, 0.0001)) These look reasonable. Now let’s simulate a comparison in outcomes for control and treatment in a city. If the control is better, we’ll say that’s “1.0” and if the treatment is better, we’ll say that’s “0.0”. Additionally, half of each mailing list gets control and half of each mailing list gets the treatment: def difference_in_purchase_rates(population, control_rate, treatment_rate): control_actual = actual_purchase_rate( population//2, control_rate) treatment_actual = actual_purchase_rate( population//2, treatment_rate) difference = 1.0 if control_actual &gt; treatment_actual else 0.0 return difference Now let’s see how that looks: for _ in range( 5): print( difference_in_purchase_rates( 100000, 0.0001, 0.00011)) Now we want to see what happens over N days: def simulate_difference_for_n_days(population, control_rate, treatment_rate, days): return [difference_in_purchase_rates( population, control_rate, treatment_rate) for i in range( days)] Let’s see what it looks like over 30 days: print( simulate_difference_for_n_days( 100000, 0.0001, 0.00011, 30)) There are a number of ways we might interpret the idea of a “streak”. A streak is N 1’s followed by a 0. For example, if N is 3, then we’re looking for something like 0, 1, 1, 1, 0. A streak is N or more 1’s followed by a 0. For example, if N is 3, then we’re looking for 0, 1, 1, 1, 0 but also 0, 1, 1, 1, 1, 0. A streak is any number of N 1’s for example if N is 3 and there are 5 1’s, then that’s 3 streaks (1, 2, 3), (2, 3, 4), (3, 4, 5). what kind of streak we’re looking for matters because it affects both identification of the event of interest and the number of possible events and thus affects our conclusions. Since probability is just counting, we need to make sure of what we’re counting. For this problem, let’s say that we’re interested in the 3rd one…we want to know whenever 3 days in a row have 1’s regardless of what happens before or after. This makes the simulation a bit easier because for a simulation of length M and a sequence of length N, there are M-N+1 such possible sequences. Let’s write a function that identifies these sequences: def count_sequences( n, data): all_sequences = [data[i:i+n] for i in range(len( data)-n+1)] streaks = sum([1.0 if sum(xs) == float(n) else 0.0 for xs in all_sequences]) return streaks, len( all_sequences) data = simulate_difference_for_n_days( 100000, 0.0001, 0.00011, 30) print( data) streak, sequences = count_sequences( 3, data) print( streak, sequences, streak/sequences) So that’s about a 7.1% chance of seeing a streak of 3 days at least once in a 30 day period in a single city. If we want to measure the average, we’d need to re-run the experiment a lot of times: streaks = [] sequences = [] for i in range( 100): data = simulate_difference_for_n_days( 100000, 0.0001, 0.00011, 30) streak, sequence = count_sequences( 3, data) streaks.append( streak) sequences.append( sequence) print( sum(streaks)/sum(sequences)) So, roughly, there’s a 5.0% chance of seeing at least one streak of 3 days in a single city over a 30 day period even if the treatment is better. But is this what we really want to know? There are 20 cities…we want to know the probability of observing such a streak in at least one of the 20 cities…so the event space is “streak-in-a-city”. def streak_in_a_city( cities, population, control_rate, treatment_rate, streak_length): results = [] for i in range( cities): data = simulate_difference_for_n_days(population, control_rate, treatment_rate, 30) streak, sequence = count_sequences( streak_length, data) result = 1.0 if streak &gt; 0 else 0.0 results.append( result) return sum(results), cities streaks, cities = streak_in_a_city( 20, 100000, 0.0001, 0.00011, 3) print( streaks, cities, streaks/cities) streaks = [] experiments = [] for i in range( 100): streak, cities = streak_in_a_city( 20, 100000, 0.0001, 0.00011, 3) streaks.append( streak) experiments.append( cities) print( sum( streaks)/sum(experiments)) As we can see, we expect to see a losing streak of 3 in some city even if the treatment is definitely better than control about 67.7% of the time. Here’s an interesting observation. Once we had the probability of 5.0% for a streak of losses in a city, did we need to do a simulation for 20 cities? The answer is, no. If we let a losing streak be “tails” then the question we’re asking about our new model and cities is, if the probability of tails is 5%, and we flip 20 coins simultaneously, what is the probability that we’ll see at least one tail? The “at least one” part is what makes it harder. However, if we reframe the question as, what is the probability of seeing 20 heads when tossing 20 coins if the probability of heads is 95%, then we have a draw from a Binomial distribution: from scipy.stats import binom binom.pmf(20, 20, 0.95) If this is the probability of all heads, then we can use the Axioms of Probability to find out the probability of 1 tail or 2 tails or 3 tails, etc, which is “at least one tail”. Since the probability of all outcomes is one, and we have the probability of the single outcome we don’t want, we can simply subtract it from 1 to get the probability of the outcomes we do want. And if we take 1 - 0.36, we get 0.64, the probability of “at least one tail”. The result is close to what we simulated. You should always be on the lookout for such shortcuts. Now try solving the problem yourself with different assumptions about what constitutes a streak or even what the true difference between the new model (treatment) and control is (the lift…which can be negative and still be called “lift”). 4.10 Conclusion We covered a lot of ground in this chapter from the basic ideas of probability to fallacies in probabilistic reasoning. Still, probability is a fundamental tool in data science. Returning to our definition of data science, it says: Data science is the application of math and computers to solve problems that stem from a lack of knowledge, constrained by the small number of people with any interest in the answers. The fundamental problem of Science is extrapolating general conclusions from specific data sets. That is, inference. Probability is our fundamental tool for dealing with the problem of inference because it allows us to model our uncertainty in a very rigorous way. Still…it’s not a panacea. It does not magically make inference deductive instead of inductive. Although the types, rules and laws of probability are important, the most important part of this chapter is Bayes Rule (or Theorem). Bayes Rule allows us tells us how to update our beliefs based on evidence and forms the basis for Statistical Inference discussed in this book. #S## Review When asked to give an example, do not use dice, coins or cards (or any gambling device). Why is a coin flip deterministic but we still need probability to model it? What is our working definition of probability? If \\(A = {a_1, a_2, a_3}\\) and \\(B = {b_1, b_2}\\), then answer the following questions: What does \\(P(A, B)\\) denote? What does \\(P(A)\\) denote? How did we arrive at it from \\(P(A, B)\\)? What does \\(P(A=a_1\\)) denote? What does \\(P(a_1)\\) denote? Why should we be careful when using a “shorthand”? What does \\(P(A|B)\\) denote? How many probability distributions does it represent? What does \\(P(A|B)P(B)\\) denote? Write it out. Express \\(P(A)\\) using the Total Probability. What is the difference between an outcome and an event? Give an example not shown elsewhere. Give an example of the independence of two outcomes. Give an example of conditional independence of three outcomes. What is the Gambler’s Fallacy? What is the Inverse Probability Fallacy? What is the Prosecutor’s Fallacy? Joe has been randomly selected for drug testing from a population that has about 3% heroin use. Joe tests positive for heroin use (\\(u\\)). The test used correctly identifies users 95% of the time \\(P(+|u) = 0.95\\) and correctly identifies non-users 90% of the time \\(P(-|c) = 0.90\\) (\\(c\\) for “clean”). What is the probability that Joe is using heroin \\(P(u|+)\\)? What are the increment and total evidences in this problem? "],
["final-words.html", "Chapter 5 Final Words", " Chapter 5 Final Words We have finished a nice book. "],
["references.html", "References", " References "]
]
