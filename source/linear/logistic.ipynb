{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import patsy\n",
    "\n",
    "import sys\n",
    "sys.path.append('resources')\n",
    "import models\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In the last section, we looked at how we can use a linear model to fit a numerical target variable like child IQ (or price or height or weight). Regardless of the method you use, this is called a *regression problem*. Linear regression is one way of solving the regression problem.\n",
    "\n",
    "When your target variable is a categorical variable, this is a *classification problem*. For now we'll work only with the case where there are two outcomes or labels.\n",
    "\n",
    "With our numerical $y$, we started with a model:\n",
    "\n",
    "$\\hat{y} = N(\\beta_0, \\sigma)$\n",
    "\n",
    "with the innovation that we could replace the mean in the normal distribution with a linear function of features, $f(X)$.\n",
    "\n",
    "We can do the same thing for classification. If we have a binary categorical variable $y$, it has a Bernoulli distribution with probability $p$ as the parameter. We can estimate $y$ as:\n",
    "\n",
    "$\\hat{y} = p$\n",
    "\n",
    "or the fraction of \"successes\" in the data. But what if we did the same thing as before? What if $p$ was a function of additional features? We would have:\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 x$\n",
    "\n",
    "and we would have a model that represented how the probability of $y$ changes as $x$ changes. Although this sounds good, there is a problem. $\\beta_0 + \\beta_1 x$ is not bounded to the range (0, 1) which we require for probabilities. But it does turn out that there is a solution: we can use a transformation to keep the value in the range (0, 1), the logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Logistic Function\n",
    "\n",
    "The logistic function is:\n",
    "\n",
    "$logistic(z) = logit^{-1}(z) = \\frac{e^z}{1 + e^z} = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "And it looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def logistic( z):\n",
    "    return 1.0 / (1.0 + np.exp( -z))\n",
    "\n",
    "figure = plt.figure(figsize=(5,4))\n",
    "\n",
    "axes = figure.add_subplot(1, 1, 1)\n",
    "\n",
    "xs = np.linspace( -10, 10, 100)\n",
    "ys = logistic( xs)\n",
    "\n",
    "axes.plot( xs, ys)\n",
    "axes.set_ylim((-0.1, 1.1))\n",
    "axes.set_title(\"Logistic Function\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter what the value of $x$, the value of $y$ is always between 0 and 1 which is exactly what we need for a probability.\n",
    "\n",
    "There are a few additional things to note at this point. First, there is not a single definition of logistic regression. Gelman defines logistic regression as:\n",
    "\n",
    "$P(y=1) = logit^{-1}(\\beta_0 + \\beta_1 x)$\n",
    "\n",
    "in terms of the inverse logit function. Such a function returns the probability that $y = 1$. There are other possibilites (for example, a general maximum entropy model).\n",
    "\n",
    "Second, interpreting the coefficients becomes a bit of a problem. Let's assume that we have no features and only have:\n",
    "\n",
    "$P(y=1) = logit^{-1}(\\beta_0)$\n",
    "\n",
    "what, exactly, is $\\beta_0$? It's not a probability because the probability interpretation only takes place once we have transformed the result using the inverse logit function. We take note of the following truism:\n",
    "\n",
    "$logit^{-1}( logit( p)) = p$\n",
    "\n",
    "This is simply what it means to be an inverse function of some other function. But the interesting thing is that this means that:\n",
    "\n",
    "$\\beta_0 = logit( p)$\n",
    "\n",
    "and we do know what $logit(p)$ is, it's the *log odds*. $logit$ is defined as:\n",
    "\n",
    "$logit(p) = log(\\frac{p}{1-p})$\n",
    "\n",
    "if $p$ is the probability of an event, then $\\frac{p}{1-p}$ is the log odds of the event (the ratio of the probability for an event and the probability against the event).\n",
    "\n",
    "The third difficulty is that the logistic regression is non-linear. For linear regression, the slopes of the curve (a line) are constant (the $\\beta$s) and while logistic regression has a linear predictor, the result is non-linear in the probability space. For example, a 0.4 point increase in log odds from 0.0 to 0.4 increases probability from 50% to 60% but a 0.4 point increase in log odds from 2.2 to 2.6 only increases probability from 90% to 93%.\n",
    "\n",
    "Finally, we lose a lot of our ability to visualize what's going on when we moved from linear regression to logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Loss does Logistic Regression minimize?\n",
    "\n",
    "Several times now, we've turned the question of variance on its head and asked, \"what estimate minimizes my error?\". For a single prediction, $\\beta_0$, of a numerical variable, we know that we want some value that minimizes Mean Squared Error (MSE):\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum(y - \\beta_0)^2$\n",
    "\n",
    "and that this means our prediction of $\\beta_0$ should be the mean, $\\bar{y}$. This is also true for *linear* regression where we minimize MSE:\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum(y - \\hat{y})^2$\n",
    "\n",
    "Also note that we can use MSE to *evaluate* linear regression (it, or some variant, is really the only way we have to evaluate linear regression's predictions).\n",
    "\n",
    "We do not use MSE for logistic regression, however, mostly because we want something with a better first derivative. Instead of MSE, we often use *cross entropy*  (also called *log loss*, we'll stick with cross entropy):\n",
    "\n",
    "$L(\\beta) = -\\frac{1}{n} \\sum y log(\\hat{y}) + (1-y) log(1-\\hat{y})$\n",
    "\n",
    "This has several implications:\n",
    "\n",
    "1. Just because \"regression\" is in the name, we do not use Mean Squared Error to derive or evaluation logistic regression.\n",
    "2. Although we do use cross entropy to derive logistic regression, we do *not* use it to evaluate logistic regression. We tend to use error rate and other metrics to evaluate it (which we will discuss in a few chapters). For now, we will just use error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Continuous Feature (Synthetic Data)\n",
    "\n",
    "As before, we're going to start with synthetic data to get our proverbial feet wet. Even here, generating synthetic data isn't as easy as it is for linear regression. We basically need to estimate the $p$ for each value of $x$ and then simulate it. The algorithm is something like this:\n",
    "\n",
    "```\n",
    "1. generate x using the standard normal distribution or binomial if categorical.\n",
    "2. for each data point:\n",
    "3.     z = beta_0 + beta_1 * x\n",
    "4.     pr = 1/(1_exp(-z))\n",
    "5.     y = 1 if rand() < pr else 0\n",
    "```\n",
    "\n",
    "Of note, the logistic function does not output $\\hat{y}$ as it does with linear regression. It outputs the estimated probability of $y=1$. We can take that probability and compare it to a threshold and assign $y = 0$ or $y = 1$. The $y$ above is the *real* y for the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(83474722)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data[\"x\"] = stats.norm.rvs(0, 1, 100)\n",
    "data[\"z\"] = 0.5 + data[\"x\"] * 0.5\n",
    "data[\"pr\"] = list(map(lambda z: logistic(z), data[\"z\"]))\n",
    "data[\"y\"] = list(map(lambda pr: 1 if np.random.uniform() < pr else 0, data[\"pr\"]))\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth taking a bit more in-depth look at this data even though it's synthetic (or *because* it's synthetic). We generated $x$ from the standard Normal distribution: $x \\sim N(0, 1)$. $z$, an intermediate step, is the actual linear model: $z = \\beta_0 + \\beta_1 x$ or $z = 0.5 + 0.5 x$.\n",
    "\n",
    "As the earlier discussion mentions, we pass $z$ through the logistic function to bound it to the interval (0, 1). The result, $pr$, represents a probability. This is a conditional probability: $P(y=1|x)$. In order to find out the \"true\" $y$ for each $x$, we simulate that probability. \n",
    "\n",
    "As we can see in the table below, we have $pr=0.663$ and $y=1$ (obs 1) as well as $pr=0.857$ and $y=0$. This is logistic regression's form of \"error\", \"noise\", or the \"known unknowns and unknown unknowns\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>z</th>\n",
       "      <th>pr</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.355745</td>\n",
       "      <td>0.677873</td>\n",
       "      <td>0.663264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.412298</td>\n",
       "      <td>0.706149</td>\n",
       "      <td>0.669550</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.830253</td>\n",
       "      <td>0.084874</td>\n",
       "      <td>0.521206</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.579133</td>\n",
       "      <td>1.789567</td>\n",
       "      <td>0.856874</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.067772</td>\n",
       "      <td>-0.033886</td>\n",
       "      <td>0.491529</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         z        pr  y\n",
       "0  0.355745  0.677873  0.663264  1\n",
       "1  0.412298  0.706149  0.669550  1\n",
       "2 -0.830253  0.084874  0.521206  1\n",
       "3  2.579133  1.789567  0.856874  0\n",
       "4 -1.067772 -0.033886  0.491529  0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use a constant model as we have before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter what $x$ is, we say there's a 57% probability that the value of $y$ is 1. Since 57% is over 50%, we could just guess that for any $x$, $\\hat{y} = 1$. We would be right 57 of the time and wrong 43% of the time on average. 43% is the model's *error rate*.\n",
    "\n",
    "Can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = models.logistic_regression(\"y ~ x\", data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: y ~ x\n",
      "-------------  ---------  -----\n",
      "Coefficients              Value\n",
      "               $\\beta_0$  0.25\n",
      "x              $\\beta_1$  0.22\n",
      "\n",
      "Metrics        Value\n",
      "Error ($\\%$)   42.00\n",
      "Efron's $R^2$  0.01\n",
      "-------------  ---------  -----\n"
     ]
    }
   ],
   "source": [
    "print(models.simple_describe_lgr(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a *little* better. The error rate here is 42% instead of (1-0.57) or 43% but that's not very encouraging. Logistic Regression doesn't actually *have* an $R^2$ metric. What we have shown here is Efron's Pseudo $R^2$. It basically measures the same thing as interpretation #1 of the \"real\" $R^2$: it's the percent of the variability in $y$ explained by the model. Not very much.\n",
    "\n",
    "Additionally, our estimates of the coefficients, $\\beta_0$ and $\\beta_1$, are pretty bad compared to the ground truth in the synthetic data. In the linear regression case we were able to recover them fairly easily. Why is the synthetic data so bad?\n",
    "\n",
    "Note that our base probability is not really much different than a coin toss (57% versus 50%). Assume a given $x$ leads to a probability of 65%. We need a lot more examples of $x$ to calculate that 65%...if we only have a few, we may never actually observe the case where $y=1$.\n",
    "\n",
    "What happens with the current data generator if we just generate more data, n=10,000 instead of n=100?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data[\"x\"] = stats.norm.rvs(0, 1, 10000)\n",
    "data[\"z\"] = 0.5 + data[\"x\"] * 0.5\n",
    "data[\"pr\"] = list(map(lambda z: logistic(z), data[\"z\"]))\n",
    "data[\"y\"] = list(map(lambda pr: 1 if np.random.uniform() < pr else 0, data[\"pr\"]))\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can re-run our logistic regression on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: y ~ x\n",
      "-------------  ---------  -----\n",
      "Coefficients              Value\n",
      "               $\\beta_0$  0.48\n",
      "x              $\\beta_1$  0.49\n",
      "\n",
      "Metrics        Value\n",
      "Error ($\\%$)   36.28\n",
      "Efron's $R^2$  0.05\n",
      "-------------  ---------  -----\n"
     ]
    }
   ],
   "source": [
    "result1 = models.logistic_regression(\"y ~ x\", data = data)\n",
    "print(models.simple_describe_lgr(result1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our coefficient estimates are almost exactly the same as the ground truth. Still our error rate is 36.3% instead of 43.0%. This is probably as good as we can get. What happens if bump up the base probability a bit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data[\"x\"] = stats.norm.rvs(0, 1, 10000)\n",
    "data[\"z\"] = 0.75 + data[\"x\"] * 10\n",
    "data[\"pr\"] = list(map(lambda z: logistic(z), data[\"z\"]))\n",
    "data[\"y\"] = list(map(lambda pr: 1 if np.random.uniform() < pr else 0, data[\"pr\"]))\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this data. The probabilities of each observation are now either very near 0 or very near 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>z</th>\n",
       "      <th>pr</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.875917</td>\n",
       "      <td>9.509174</td>\n",
       "      <td>0.999926</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.537053</td>\n",
       "      <td>-4.620530</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.809259</td>\n",
       "      <td>8.842593</td>\n",
       "      <td>0.999856</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.632188</td>\n",
       "      <td>-5.571878</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.757485</td>\n",
       "      <td>-6.824855</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         z        pr  y\n",
       "0  0.875917  9.509174  0.999926  1\n",
       "1 -0.537053 -4.620530  0.009752  0\n",
       "2  0.809259  8.842593  0.999856  1\n",
       "3 -0.632188 -5.571878  0.003789  0\n",
       "4 -0.757485 -6.824855  0.001085  0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constant model shows a probability of 52.2% for $y=1$. This means it has an error rate of 47.8%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5215"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about our logistic regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: y ~ x\n",
      "-------------  ---------  -----\n",
      "Coefficients              Value\n",
      "               $\\beta_0$  0.69\n",
      "x              $\\beta_1$  9.45\n",
      "\n",
      "Metrics        Value\n",
      "Error ($\\%$)   5.09\n",
      "Efron's $R^2$  0.85\n",
      "-------------  ---------  -----\n"
     ]
    }
   ],
   "source": [
    "result2 = models.logistic_regression(\"y ~ x\", data = data)\n",
    "print(models.simple_describe_lgr(result2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients are almost exact *and* the error rate is only 5.1%. The (pseudo) $R^2$ shows that our model explains 85% of the variation in $y$.\n",
    "\n",
    "This set of experiments shows us a number of things. First, generating synthetic data is very useful for learning how your algorithms work. In fact, let's do one more experiment. Let's reduce the number of observations back to 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data[\"x\"] = stats.norm.rvs(0, 1, 10000)\n",
    "data[\"z\"] = 0.75 + data[\"x\"] * 10\n",
    "data[\"pr\"] = list(map(lambda z: logistic(z), data[\"z\"]))\n",
    "data[\"y\"] = list(map(lambda pr: 1 if np.random.uniform() < pr else 0, data[\"pr\"]))\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: y ~ x\n",
      "-------------  ---------  -----\n",
      "Coefficients              Value\n",
      "               $\\beta_0$  0.64\n",
      "x              $\\beta_1$  8.93\n",
      "\n",
      "Metrics        Value\n",
      "Error ($\\%$)   5.77\n",
      "Efron's $R^2$  0.83\n",
      "-------------  ---------  -----\n"
     ]
    }
   ],
   "source": [
    "result3 = models.logistic_regression(\"y ~ x\", data = data)\n",
    "print(models.simple_describe_lgr(result3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our error rate is still quite a bit lower but the estimates of our coefficients aren't as good. We need both a lot of data and clear underlying pattern *and* this pattern isn't as obvious as it is with linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Real Data\n",
    "\n",
    "When it comes to either numerical features or a binary categorical features, there is no difference between linear regression and logistic regression. We can have numerical features which will affect the slope and intercept of the line. We can have binary categorical features that will affect the intercept of the line. We can have interaction terms that will affect the slope of the line.\n",
    "\n",
    "The main difference with linear regression is in the interpretation of the coefficients.\n",
    "\n",
    "For logistic regression, the coefficients are log-odds and while some people are quite comfortable thinking in terms of log-odds, most are not. How do we convert them into something we can understand?\n",
    "\n",
    "Let's begin the discussion by looking at real data. This data is from a study of villager behavior in Bangladesh. Wells were examined for natural arsenic contamination and villagers using wells with higher arsenic readings were encouraged to use other wells or dig new ones. The variables are:\n",
    "\n",
    "* **switch** - yes (1) or no (0), did the respondent switch to a new well.\n",
    "* **dist** - distance to the nearest safe well in meters.\n",
    "* **arsenic** - arsenic level of the respondentâ€™s well.\n",
    "* **assoc** - does the respondent or a family member belong to a community association.\n",
    "* **educ** - the educational attainment of the respondent in years.\n",
    "\n",
    "Let's start out with a logistic regression model for $\\hat{switch}$:\n",
    "\n",
    "$P(\\hat{switch}=1) = logistic^{-1}(\\beta_0 + \\beta_1 dist)$\n",
    "\n",
    "although we really have something like:\n",
    "\n",
    "$z = \\beta_0 + \\beta_1 dist$\n",
    "\n",
    "$\\hat{pr} = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "$\\hat{y}$ = 1 if $\\hat{pr}$ > 0.5 else 0\n",
    "\n",
    "which is a bit more complicated to write each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells = pd.read_csv( \"resources/arsenic.wells.tsv\", sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3020 entries, 1 to 3020\n",
      "Data columns (total 5 columns):\n",
      "switch     3020 non-null int64\n",
      "arsenic    3020 non-null float64\n",
      "dist       3020 non-null float64\n",
      "assoc      3020 non-null int64\n",
      "educ       3020 non-null int64\n",
      "dtypes: float64(2), int64(3)\n",
      "memory usage: 141.6 KB\n"
     ]
    }
   ],
   "source": [
    "wells.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing particularly startling here. Let's see a few values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>switch</th>\n",
       "      <th>arsenic</th>\n",
       "      <th>dist</th>\n",
       "      <th>assoc</th>\n",
       "      <th>educ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16.826000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.71</td>\n",
       "      <td>47.321999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2.07</td>\n",
       "      <td>20.966999</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.15</td>\n",
       "      <td>21.486000</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1.10</td>\n",
       "      <td>40.874001</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   switch  arsenic       dist  assoc  educ\n",
       "1       1     2.36  16.826000      0     0\n",
       "2       1     0.71  47.321999      0     0\n",
       "3       0     2.07  20.966999      0    10\n",
       "4       1     1.15  21.486000      0    12\n",
       "5       1     1.10  40.874001      1    14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wells.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model (and error rate) are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(switch=1) = 0.58 (0.42)\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(wells.switch)\n",
    "print(\"P(switch=1) = {0:.2f} ({1:.2f})\".format(mean, 1-mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model (often called the \"null\" model) is that $P(switch=1) = 0.58$ which leads to an error rate of 42%. Let's see what logistic regression can get us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: switch ~ dist\n",
      "-------------  ---------  -----\n",
      "Coefficients              Value\n",
      "               $\\beta_0$  0.60\n",
      "dist           $\\beta_1$  -0.01\n",
      "\n",
      "Metrics        Value\n",
      "Error ($\\%$)   40.53\n",
      "Efron's $R^2$  0.01\n",
      "-------------  ---------  -----\n"
     ]
    }
   ],
   "source": [
    "result = models.logistic_regression( \"switch ~ dist\", data = wells)\n",
    "print(models.simple_describe_lgr(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So again, this is the real world and real data and sometimes you only get improvements such as these. Despite the showing on this data, logistic regression is a very powerful modeling technique.\n",
    "\n",
    "We can see that the error rate and (pseudo) $R^2$ of this model aren't great but we're much more interested in interpreting the model coefficients. What do they mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intercept\n",
    "\n",
    "The intercept in this case has a legitimate $dist = 0$ interpretation. If the alternative well is 0 meters away, what is the probability of switching?\n",
    "\n",
    "We can use our previous identity and use the inverse logit (logistic) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6465252043640823"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic( 0.6038)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the probability of switching is 64.7% if the safe well is zero meters away (that doesn't really bode well). If we were to run the logistic regression without any regressors, we could think of $\\beta_0$ as the *prior* log odds. In essence, logistic regression is a function that calculates conditional probabilities based on the features instead of using a table. \n",
    "\n",
    "Once you add features, $\\beta_0$ is no longer a pure prior because it has been optimized in the presence of the other features and therefore is still a conditional probability...just with all the features at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficients\n",
    "\n",
    "Next, we can look at each coefficient (or in this case, the only coefficient). The model basically says there is a 0.0062 decrease in *log odds* (remember that the coefficients are not in probability space until transformed) for every meter to the nearest safe well. Now we have a problem. While $\\beta_i$ as *log odds* is linear in $x_i$ (that's *all* the term \"linear model\" means), arbitrary transformations of $\\beta_i$, $t(\\beta_i)$ is not necessarily linear in $x_i$ and that is the case here. How do we get around this problem? \n",
    "\n",
    "\n",
    "There are several options:\n",
    "\n",
    "**No. 1 - Evaluate at the mean of the variable with a unit change.**\n",
    "\n",
    "The mean value of dist(ance) is 48.33. If we evaluate our model using that value, we get:\n",
    "\n",
    "$P(switch = 1)$ = $logit^{-1}(0.6038 - 0.0062 \\times 48.33)$ = $logit^{-1}(0.304154)$ = 0.5755\n",
    "\n",
    "And if we do the same thing again after adding 1 meter to the average distance, we get:\n",
    "\n",
    "$P(switch = 1)$ = $logit^{-1}(0.6038 - 0.0062 \\times 49.33)$ = $logit^{-1}(0.297954)$ = 0.5739\n",
    "\n",
    "So...that's a decrease of about 0.0016 percentage points (or 0.27%) which isn't huge but then we only increased the difference by a little over 3 feet!\n",
    "\n",
    "But, you need to be careful with mean scaled data (which we'll talk about in the next chapter). A unit change is equal to one entire standard deviation which may be an extremely large value...or an extremely small one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5754576812550608 0.5739422790895422\n",
      "0.0015154021655185979\n"
     ]
    }
   ],
   "source": [
    "a = logistic( 0.6038 - 0.0062 * 48.33)\n",
    "b = logistic( 0.6038 - 0.0062 * 49.33)\n",
    "print(a, b)\n",
    "print(a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have more than one $x_i$, you should set all of them to their mean values and then do a unit change for each variable separately.\n",
    "\n",
    "Note that this is a good reason to mean *center* data in a logistic regression but not to mean *scale* it. The reason for not mean scaling is that:\n",
    "\n",
    "1. The coefficients do have clear interpretations and relative magnitudes (changes in probability after transformation).\n",
    "2. Mean *scaling* makes 1 unit equal to one standard deviation of the standard normal distribution. This might be a very, very large value in the variables actual domain which messes up the approximation.\n",
    "\n",
    "**No. 2 - Calculate the derivative of the logistic and evaluate it at the mean of the variable.**\n",
    "\n",
    "$\\frac{\\partial}{\\partial x_i}logit^{-1}(\\beta X) = \\frac{\\beta_i e^{\\beta X}}{(1 + e^{\\beta X})^2}$\n",
    "\n",
    "but $\\beta X$ (z) is just the log odds at the mean values of X (if X is indeed $[1.0, \\bar{x}_1, \\bar{x}_2, ... ,\\bar{x}_n]$) so if we plug our value for the model evaluated at the mean into the derivative, we get:\n",
    "\n",
    "$\\frac{0.0062 e^{0.0062 \\times 0.3042}}{(1 + e^{0.0062 \\times 0.3042})^2} = 0.0016$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0015499986216064004\n"
     ]
    }
   ],
   "source": [
    "def logistic_slope_at( beta, z):\n",
    "    return (beta * np.exp( beta * z)) / (1.0 + np.exp( beta * z))**2\n",
    "\n",
    "print(logistic_slope_at( 0.0062, 0.3042))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No. 3 - Divide by 4 Rule**\n",
    "\n",
    "$\\beta_1 / 4 = 0.0062 / 4 = 0.00155$\n",
    "\n",
    "It's just a rule of thumb but easy. It has the same general interpretation though...the change in probability from a unit change at the mean of the regressor. Again, this approach can get very funky with mean scaled data because a unit change is a full standard deviation which can actually be enormous or infinitesimal. Why does that work?\n",
    "\n",
    "The slope of the logistic curve is maximized where the first derivative is zero or $\\beta_0 + \\beta_1 + x = 0$. We can solve for this as:\n",
    "\n",
    "\n",
    "$\\frac{beta_1e^0}{(1+e^0)^2}$\n",
    "\n",
    "$\\frac{\\beta_1 \\times 1}{(1+1)^2}$\n",
    "\n",
    "$\\frac{\\beta_1}{4}$\n",
    "\n",
    "This interpretation holds best in the context of mean values for the corresponding feature, $x$.\n",
    "\n",
    "**No. 4 - Average Predictive Difference**\n",
    "\n",
    "We can also average the probabilities over all of our data points for a specific change in each of our predictors. For a model with only one predictor, this amounts to the same thing as No. 1 so we will save this for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Transformations\n",
    "\n",
    "We'll talk more about transformations in the next chapter. The main point of this chapter is to establish what linear and logistic regression are and how to interpret them. However, there is an especially useful transformation for logistic regression and that involves transforming the units.\n",
    "\n",
    "This doesn't affect the quality of the model at all. It does, however, change how you interpret it. For example, the current units of $dist$ are meters. The probability of change per *meter* is pretty small. But what about the probability of changing per *ten meters*? That's nearly 33 feet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells[\"dist10\"] = wells[\"dist\"]/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: switch ~ dist10\n",
      "-------------  ---------  -----\n",
      "Coefficients              Value\n",
      "               $\\beta_0$  0.60\n",
      "dist10         $\\beta_1$  -0.06\n",
      "\n",
      "Metrics        Value\n",
      "Error ($\\%$)   40.53\n",
      "Efron's $R^2$  0.01\n",
      "-------------  ---------  -----\n"
     ]
    }
   ],
   "source": [
    "result = models.logistic_regression( \"switch ~ dist10\", data = wells)\n",
    "print(models.simple_describe_lgr(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now reinterpret the model. The probability of switching decreases by 1.5% (-00.619/4 = -0.015475 or \"Divide by 4\" rule) at the average when the distance to the safe well increases by 10 meters.\n",
    "\n",
    "It's interesting to note that Gelman used 100 to scale his data. The problem with this, in an interpetability sense, is that if you look at distance, the median distance to a safe well is 36.7 meters. The 3rd quartile is 64 meters. A 100 meter difference just doesn't figure prominently into the data even though the maximum distance was 339.5 meters. 10 meters seems like a reasonable unit in this case.\n",
    "\n",
    "Again, this doesn't change how good the model is. But it does it easier to talk about than \"15 100ths of a percent per meter\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Logistic Regression\n",
    "\n",
    "It's not quite as easy to plot a logistic regression as it is linear regression. If we just plot the data, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(10,6))\n",
    "\n",
    "axes = figure.add_subplot(1, 1, 1)\n",
    "\n",
    "xs = wells[ \"dist10\"]\n",
    "ys = wells[ \"switch\"]\n",
    "axes.scatter( xs, ys, color=\"dimgray\", alpha=0.5)\n",
    "betas = result[ \"coefficients\"]\n",
    "\n",
    "zs = np.linspace( xs.min(), xs.max(), 100)\n",
    "ps = [logistic( betas[ 0] + betas[ 1] * x) for x in zs]\n",
    "\n",
    "axes.plot(zs, ps, '-', color=\"firebrick\", alpha=0.75)\n",
    "axes.set_title( result[ \"formula\"])\n",
    "axes.set_xlabel(\"dist (10s of meters)\")\n",
    "axes.set_ylabel(\"switch\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's just not very interesting or informative on its own. Additionally, you have the problem that logistic regression is nonlinear. We'll get into plotting multivariate regression (linear and logistic) in the next chapter. The solution for logistic regression is usually to plot the *decision boundary* in feature space and not to plot the target at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Inference\n",
    "\n",
    "As with linear regression, we can also apply bootstrap inference to logistic regression and with the same results. Here we only show the function in operation. We also make the innovation that we include \"divide by 4\" interpretations of our coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: switch ~ dist10\n",
      "-------------  ---------  ------  -------  ------  ------\n",
      "                                  95% BCI\n",
      "Cofficients               Mean    Lo       Hi      P(y=1)\n",
      "               $\\beta_0$  0.604   0.493    0.715   0.647\n",
      "dist10         $\\beta_1$  -0.062  -0.081   -0.045  -0.015\n",
      "\n",
      "Metrics        Mean       Lo      Hi\n",
      "Error ($\\%$)   40.530     38.970  42.302\n",
      "Efron's $R^2$  0.014      0.007   0.022\n",
      "-------------  ---------  ------  -------  ------  ------\n"
     ]
    }
   ],
   "source": [
    "result = models.bootstrap_logistic_regression(\"switch ~ dist10\", wells)\n",
    "print(models.describe_bootstrap_lgr(result, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret this table? Let's look at \"dist10\" (AKA $\\beta_1$).\n",
    "\n",
    "The mean value is -0.062 *log odds*. Every ten meters we move away from the better well, the *log odds* of switching decrease by 0.062. Looking at \"Lo\" and \"Hi\", there is a 95% probability the value is in the range of -0.081 and 0.045. Put differently, and perhaps a bit more Bayesian, the data are consistent with a range of values for \"dist10\". The values having at least 95% credibility, are in the range -0.081 and -0.045. Finally, using the \"divide by 4\" rule, the probability of switching, P(y=1), decreases by 1.5 percentage *points* for every 10 meters we move away from the better/safe well.\n",
    "\n",
    "If we had the visualization skills, we might just show a tiny histogram of the posteriors for each coefficient/metric instead of using intervals.\n",
    "\n",
    "The development so far has been pedagogical. You should always do bootstrap inference for both linear and logistic regression and going forward, we will."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More than Two Outcomes\n",
    "\n",
    "Binary classification shows up quite a bit: does it fail or not, does he have a heart attack or not, does she purchase it or not, does he click on it or not. But in many instances, the event is not actually binary. Death is inevitable. It's not \"does he have a heart attack or not\" but \"does he die from a heart attack, cancer, flu, traffic accident, ...\". It's not \"does she purchase it or not\" but \"does she buy the shoes, the shop-vac, the weedwacker, ...\". It's not \"does he click on it or not\" but \"does he click on this, does he click on that, does he go back, ...\".\n",
    "\n",
    "This gives us some clue as to how to deal with multiclass classification problems.\n",
    "\n",
    "First, there are classification algorithms that *can* deal with multiclass problems \"directly\". Decision trees are a good example.\n",
    "\n",
    "Second, every algorithm that can handle only binary classification can also be made to handle multiclass classification. If the response variable has $n$ possible outcomes then you can train $n$ binary models where each is trained on \"the class\" and \"not the class\". For example, if there were three classes: buy, sell, hold. Then you first convert your data (temporarily) into \"buy/not buy\" and train a model, then convert to \"sell/not sell\" and train a model, then convert to \"hold/not hold\" and train a model:\n",
    "\n",
    "$z_{buy} = \\beta^{buy}_0 + \\beta^{buy}_1 x_1$\n",
    "\n",
    "from data where the classes are now \"buy/don't buy\".\n",
    "\n",
    "$z_{sell} = \\beta^{sell}_0 + \\beta^{sell}_1 x_1$\n",
    "\n",
    "from data where the classes are now \"sell/don't sell\"\n",
    "\n",
    "$z_{hold} = \\beta^{hold}_0 + \\beta^{hold}_1 x_1$\n",
    "\n",
    "from data where the classes are now \"hold/don't hold\"\n",
    "\n",
    "This works as long as each model has the same $x_1$ (and this generalizes to more than one feature: $x_1$, $x_2$,...$x_n$). Actually, multinomial logistic regression does this under the covers for you...but things like Support Vector Machines do not and you do have to do it manually.\n",
    "\n",
    "You now have a metamodel for multiclass classification. When you need to make a prediction, you use all three models and pick the class that has the highest probability. Strangely, this is essentially what neural networks must do as well.\n",
    "\n",
    "Note that there's a different although related problem of *multilabel* classification. In this case, each observation might be assigned more than one outcome For example, a story might be fiction *and* sports while another one might non-fiction and sports. A discussion of this problem is beyond the scope of these notes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en685648)",
   "language": "python",
   "name": "en685648"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "189px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
